{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Signal_generator.generate_signal import generate_X_matrix\n",
    "from Algorithmes.beamforming import beamforming_method\n",
    "from Signal_generator.generate_signal import generate_A_matrix\n",
    "from Signal_generator.generate_signal import generate_S_matrix\n",
    "from Signal_generator.generate_signal import generate_noise\n",
    "from Signal_generator.generate_signal import generate_R_hat\n",
    "from Signal_generator.generate_signal import generate_R_hat_with_phase\n",
    "from Algorithmes.music import music_method\n",
    "from Algorithmes.music import estimate_angles\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torchsummary import summary\n",
    "from numpy.linalg import eigh\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from Algorithmes.music import generate_steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbSources = 1 # Nombre de sources  dans l'article ils fixent le nombre de sources à 2\n",
    "nbSensors = 9 # Nombre de capteurs\n",
    "nbTimePoints = 100 # Nombre de points temporels\n",
    "\n",
    "\n",
    "\n",
    "#Pour l'échantillon d'entraînements ils utilisent une observation de chaque combinaison d'angles d'arrivée possible pour chaque SNR level, dans le cas d'une seule source, 2G+1 observations\n",
    "#SNR ratio dans le papier est seulement low, de -20 à 0 DB avec un pas de 5\n",
    "#Petite note : si on fait un modèle pour chaque SNR level, les performances augmentent légérements, mais cela ne vaut pas l'effort effectué\n",
    "signal_noise_ratio = 3 # Rapport signal sur bruit en décibels. Si 'False', cela revient à une absence totale de bruit.\n",
    "L = nbSensors\n",
    "T = 100\n",
    "correlation_List = [] # Liste des corrélations. Il y a une corrélation nécéssaire pour chaque paire distincte de sources différentes: 0 pour 1 source, 1 pour 2 sources, 3 pour 3 sources, 6 pour 4 sources etc...\n",
    "# Ordre de remplisage de la correlation_List: de gauche à droite et ligne par ligne, moitié haut-droite de la matrice uniquement, puis symétrie de ces valeurs pour la moitié bas-gauche.\n",
    "SNR_TRAIN = 15\n",
    "Q = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrum(X, nbSensors, nbSources, print_angles = False, draw_plot = False):\n",
    "    angles_range = np.linspace(-45, 45, 2) # Angles que l'on souhaite tester\n",
    "    R_hat = generate_R_hat(X)\n",
    "\n",
    "    # Calculer les vecteurs propres et les valeurs propres\n",
    "    _, eigenvectors = np.linalg.eigh(R_hat)\n",
    "    # Sélectionner les k plus grandes valeurs propres\n",
    "    noise_subspace = eigenvectors[:, :nbSources]\n",
    "\n",
    "    # Calcul du spectre MUSIC\n",
    "    music_spectrum = np.zeros_like(angles_range, dtype=float)\n",
    "    for idx, theta in enumerate(angles_range):\n",
    "        steering_vector = generate_steering_vector(nbSensors, theta)\n",
    "        music_spectrum[idx] = 1 / np.linalg.norm(noise_subspace.conj().T @ steering_vector)\n",
    "    return music_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "def generate_deepmusic_partitioned_data(nbSensors, nbSources, T, num_samples, SNR_TRAIN, Q):\n",
    "    training_data_sets = [[] for _ in range(Q)]\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Générer un échantillon de signal reçu X\n",
    "        thetaList = np.random.uniform(-90, 90, nbSources).tolist()\n",
    "        varList = np.ones(nbSources).tolist()\n",
    "        correlation_List = np.zeros(nbSources * (nbSources - 1) // 2).tolist()\n",
    "        X = generate_X_matrix(nbSources, nbSensors, T, thetaList, varList, correlation_List, SNR_TRAIN)\n",
    "\n",
    "        # Calculer la matrice de covariance R_hat pour X\n",
    "        R_hat = generate_R_hat_with_phase(X)\n",
    "\n",
    "        # Calculer le spectre MUSIC\n",
    "        music_spectrum = create_spectrum(X, nbSensors, nbSources)\n",
    "\n",
    "        # Partitionner le spectre MUSIC en Q segments\n",
    "        spectrum_segments = np.array_split(music_spectrum, Q)\n",
    "\n",
    "        # Préparer les données d'entrée et de sortie pour chaque partition\n",
    "        for q in range(Q):\n",
    "            input_data = R_hat # Utiliser la matrice de covariance aplatie comme données d'entrée\n",
    "            output_data = spectrum_segments[q]\n",
    "            training_data_sets[q].append((input_data, output_data))\n",
    "\n",
    "    return training_data_sets\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "num_samples = 100  # Nombre d'échantillons de données à générer\n",
    "training_dataset = generate_deepmusic_partitioned_data(nbSensors, nbSources, T, num_samples, SNR_TRAIN, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "def create_dataloaders(training_data_sets, batch_size=32, split_ratio=(0.7, 0.15, 0.15)):\n",
    "    dataloaders = []\n",
    "\n",
    "    for data_set in training_data_sets:\n",
    "        inputs = torch.tensor([item[0] for item in data_set]).float()\n",
    "        outputs = torch.tensor([item[1] for item in data_set]).float()\n",
    "\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(inputs, outputs, test_size=split_ratio[1] + split_ratio[2])\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=split_ratio[2] / (split_ratio[1] + split_ratio[2]))\n",
    "\n",
    "        train_dataset = CustomDataset(X_train, y_train)\n",
    "        val_dataset = CustomDataset(X_val, y_val)\n",
    "        test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        dataloaders.append((train_loader, val_loader, test_loader))\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "# Utilisation de la fonction\n",
    "batch_size = 32\n",
    "dataloaders = create_dataloaders(training_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepMusicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepMusicModel, self).__init__()\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the fourth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Assuming the spatial dimensions (height and width) are reduced to 1x1 after the convolutions\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=20736, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=1)  # Adjusted output size\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Softmax layer \n",
    "        #self.softmax = nn.softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Apply the second convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Apply the third convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Apply the fourth convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Apply the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Apply the dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #Apply the softmax layer\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        # Apply the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = DeepMusicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloaders[1][0]\n",
    "val_loader = dataloaders[1][1]\n",
    "test_loader = dataloaders[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3, 9, 9]) torch.Size([15, 1]) torch.Size([15, 1])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in test_loader:\n",
    "    print(X.shape,Y.shape,model(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition des hyperparamètres, choisis en fonction de l'article DOA estimator\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # à monter à 200 pour le training final selon l'article\n",
    "loss_fn = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#Définition du device\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 16.396061698595684\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.210290 \n",
      " RMSE : 0.000000\n",
      "Epoch 2, Loss: 21.219645818074543\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.198372 \n",
      " RMSE : 0.000000\n",
      "Epoch 3, Loss: 16.195924719174702\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.187316 \n",
      " RMSE : 0.000000\n",
      "Epoch 4, Loss: 16.552747805913288\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.176706 \n",
      " RMSE : 0.000000\n",
      "Epoch 5, Loss: 16.1585879723231\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.166184 \n",
      " RMSE : 0.000000\n",
      "Epoch 6, Loss: 18.626880645751953\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.154383 \n",
      " RMSE : 0.000000\n",
      "Epoch 7, Loss: 34.25326029459635\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.141074 \n",
      " RMSE : 0.000000\n",
      "Epoch 8, Loss: 18.46523380279541\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.128315 \n",
      " RMSE : 0.000000\n",
      "Epoch 9, Loss: 16.00269768635432\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.115288 \n",
      " RMSE : 0.000000\n",
      "Epoch 10, Loss: 28.95483144124349\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 100.0%, Avg loss: 1.101849 \n",
      " RMSE : 0.000000\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (988968604.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[140], line 38\u001b[1;36m\u001b[0m\n\u001b[1;33m    return test_loss, correct\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        model.eval()\n",
    "        size = len(test_loader.dataset)\n",
    "        num_batches = len(test_loader)\n",
    "        test_loss, correct, MSE = 0, 0, 0\n",
    "\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                test_loss += criterion(pred, y).item()\n",
    "            \n",
    "                _, angles_predit = pred.topk(nbSources, dim=1)\n",
    "                _,angles_correct=top_values, top_indices = y.topk(nbSources, dim=1)\n",
    "                correct += (angles_predit==angles_correct).type(torch.float).sum().item()\n",
    "                #Calcul de la somme des carrés des différences entre prédictions et réalité pour la RMSE\n",
    "                for angle in range(nbSources):\n",
    "                    MSE+=(angles_correct[angle]-angles_predit[angle])**2\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        RMSE=math.sqrt(MSE/(len(test_loader)*nbSources))\n",
    "        print(f\"\\nTest set: \\n  Accuracy: {(100 * correct): >0.1f}%, Avg loss: {test_loss: >8f} \\n RMSE : {RMSE: >8f}\")\n",
    "return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 600]) torch.Size([32, 1])\n",
      "torch.Size([32, 600]) torch.Size([32, 1])\n",
      "torch.Size([6, 600]) torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            print(labels.shape,outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
