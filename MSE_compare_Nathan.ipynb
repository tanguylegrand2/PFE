{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import du script de comparaison\n",
    "from Comparison_scripts.monte_carlo_compare import run_comparison\n",
    "# from Comparison_scripts.monte_carlo_compare import run_dl_model_comparison\n",
    "from Signal_generator.generate_signal import *\n",
    "import torch.nn.functional as F\n",
    "from Algorithmes.music import estimate_angles\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMusicModel(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(DeepMusicModel, self).__init__()\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the fourth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Assuming the spatial dimensions (height and width) are reduced to 1x1 after the convolutions\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=20736, out_features=output_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Softmax layer \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Apply the second convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Apply the third convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Apply the fourth convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Apply the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Apply the dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #Apply the softmax layer\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.compare import *\n",
    "models_deep_music1 = load_deep_music(\"DeepMusic_subregion_1\", model_class=DeepMusicModel, outputs=[121])\n",
    "models_deep_music2 = load_deep_music(\"DeepMusic_subregion_2\", model_class=DeepMusicModel, outputs=[60, 61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_music_spectrum(segments, Q, N):\n",
    "    reconstructed_spectrum = np.zeros(N)\n",
    "    \n",
    "    for q, segment in enumerate(segments):\n",
    "        start_index = q * (N // Q)\n",
    "        end_index = (q + 1) * (N // Q) if q != Q - 1 else N\n",
    "\n",
    "        # Détacher le tenseur du graphe de calcul et le convertir en un tableau numpy\n",
    "        segment = segment.detach().numpy()\n",
    "        reconstructed_spectrum[start_index:end_index] = segment\n",
    "\n",
    "    return reconstructed_spectrum\n",
    "\n",
    "def deep_music_pred_1(X, nbSensors, nbSources):\n",
    "    # Générer la matrice R_hat_with_phase et la convertir en tenseur PyTorch\n",
    "    input_tensor = torch.tensor(generate_R_hat_with_phase(X), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Passer le modèle en mode évaluation et faire la prédiction\n",
    "    models_deep_music1[0].eval()\n",
    "    with torch.no_grad():\n",
    "        a = models_deep_music1[0](input_tensor)\n",
    "\n",
    "        # Détacher le tenseur du graphe de calcul et le convertir en tableau NumPy\n",
    "        a_numpy = a.detach().cpu().numpy()\n",
    "\n",
    "        # Estimer les angles à partir du spectre MUSIC\n",
    "        estimated_angles = estimate_angles(nbSources, a_numpy, np.linspace(-60, 60, 121))\n",
    "\n",
    "        return estimated_angles\n",
    "\n",
    "def deep_music_pred_2(X, nbSensors, nbSources):\n",
    "    input_tensor = torch.tensor(generate_R_hat_with_phase(X), dtype=torch.float32).unsqueeze(0)\n",
    "    a = []\n",
    "    models_deep_music2[0][0].eval()\n",
    "    models_deep_music2[0][1].eval()\n",
    "    with torch.no_grad():\n",
    "        q1 = models_deep_music2[0][0](input_tensor)\n",
    "        q2 = models_deep_music2[0][1](input_tensor)\n",
    "\n",
    "        # Détacher le tenseur du graphe de calcul et le convertir en tableau NumPy\n",
    "        q1_numpy = q1.detach().cpu().numpy()\n",
    "        q2_numpy = q2.detach().cpu().numpy()\n",
    "\n",
    "        a.append(q1)\n",
    "        a.append(q2)\n",
    "\n",
    "        # Estimer les angles à partir du spectre MUSIC\n",
    "        a = reconstruct_music_spectrum(a, 2, 121)\n",
    "        estimated_angles = estimate_angles(nbSources, a, np.linspace(-60, 60, 121))\n",
    "    \n",
    "    return estimate_angles(nbSources, a, np.linspace(-60, 60, 121))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepMusicModel(\n",
       "  (conv1): Conv2d(3, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=20736, out_features=60, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_deep_music2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithmes à importer\n",
    "from Algorithmes.beamforming import beamforming_method\n",
    "from Algorithmes.music import music_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel des algorithmes à comparer. clé: nom de l'algorithme, valeur: fonction de l'algorithme\n",
    "# La fonction doit prendre en entrée les paramètres suivants: X, nbSensors, nbSources\n",
    "# La fonction doit donner en sortie une liste de la forme [angleEstime1, angleEstime2] rangée dans l'ordre croissant\n",
    "algorithms_to_compare = {\n",
    "    \"Beamforming\": beamforming_method,\n",
    "    \"DeepMusic_subregions_1\": deep_music_pred_1,\n",
    "    \"DeepMusic_subregions_2\": deep_music_pred_2\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 100 # Nombre d'itérations de Monte Carlo pour chaque algorithme\n",
    "nbSources = 2\n",
    "nbSensors = 9\n",
    "theta = [-20, 20] # Les deux angles d'origine des sources\n",
    "two_symmetrical_angles = True # Si True, on a deux angles symétriques par rapport à 0.\n",
    "# Favoriser de angles symmétriques : cela aura un effet similaire à doubler nb_itération sans doubler le temps de calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs neutres pour les 5 paramètres à slider:\n",
    "# nbTimePoints = 100\n",
    "# var_ratio = [1] -> les deux signaux ont la même variance\n",
    "# correlation = [0] -> les deux signaux sont indépendants\n",
    "# snr = 10 -> le bruit est 10 fois plus petit que le signal\n",
    "# perturbation_parameter_sd = 0 -> les capteurs sont parfaitement alignés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples de ranges intéressantes pour les paramètres à slider:\n",
    "# var_ratio = [[1], [10], [50]]\n",
    "# correlation = [[0], [0.9], [0.95], [0.99]]\n",
    "# snr = [-10, -5, 0, 5, 10, 20]\n",
    "# perturbation_parameter_sd = [0, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT : le paramètre défini en temps que parameter_to_compare doit être sous forme d'une liste de valeurs. Exemples dans la cellule au dessus.\n",
    "# Pour var_ratio et correlation, faites attention à bien avoir une liste de liste de réels s'ils sont en parameter_to_compare, et une liste de réels sinon.\n",
    "nbTimePoints = 100\n",
    "var_ratio = [1]\n",
    "correlation = [0]\n",
    "snr = [-10, 0, 10, 20]\n",
    "perturbation_parameter_sd = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les parameter_to_compare valides sont: \"snr\", \"nbTimePoints\", \"correlation\", \"var_ratio\" et \"perturbation_parameter_sd\"\n",
    "parameter_to_compare = \"snr\" # Paramètre à faire slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour snr = -10 :\n",
      "-----\n",
      "Temps moyen d'estimation pour une itération de Beamforming: 0.0893 secondes\n",
      "Beamforming: 8 outliers removed (8.00%)\n",
      "Valeur pour Beamforming : 2.336793478260867\n",
      "-----\n",
      "Temps moyen d'estimation pour une itération de DeepMusic_subregions_1: 0.0058 secondes\n",
      "DeepMusic_subregions_1: 98 outliers removed (98.00%)\n",
      "Valeur pour DeepMusic_subregions_1 : 13.0\n",
      "-----\n",
      "Temps moyen d'estimation pour une itération de DeepMusic_subregions_2: 0.0078 secondes\n",
      "DeepMusic_subregions_2: 97 outliers removed (97.00%)\n",
      "Valeur pour DeepMusic_subregions_2 : 4.5\n",
      "-----\n",
      "Valeur de la Cramer Rao Lower Bound : 1.00842601664876\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrun_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameter_to_compare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithms_to_compare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbSources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbSensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbTimePoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrelation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbation_parameter_sd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwo_symmetrical_angles\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sven\\Documents\\Ensai 3A\\PFE\\PFE\\Comparison_scripts\\monte_carlo_compare.py:52\u001b[0m, in \u001b[0;36mrun_comparison\u001b[1;34m(parameter_to_compare, algorithms_to_compare, nbiterations, nbSources, nbSensors, theta, nbTimePoints, snr, correlation, var_ratio, perturbation_parameter_sd, two_symetrical_angles)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, algorithm_function \u001b[38;5;129;01min\u001b[39;00m algorithms_to_compare\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     51\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Début de mesure du temps\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     estimated_angles \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbSensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbSources\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Fin de mesure du temps\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     algorithm_estimations[name]\u001b[38;5;241m.\u001b[39mappend(estimated_angles)\n",
      "File \u001b[1;32mc:\\Users\\Sven\\Documents\\Ensai 3A\\PFE\\PFE\\Algorithmes\\beamforming.py:17\u001b[0m, in \u001b[0;36mbeamforming_method\u001b[1;34m(X, nbSensors, nbSources, print_angles, draw_plot)\u001b[0m\n\u001b[0;32m     15\u001b[0m     w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(inv_R_hat, steering_vector_angle) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mconj(steering_vector_angle), np\u001b[38;5;241m.\u001b[39mdot(inv_R_hat, steering_vector_angle))\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Calcul de la sortie du beamformer pour cette direction\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     beamformer_output[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mconj(w), \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Normalisation pour une meilleure visualisation\u001b[39;00m\n\u001b[0;32m     20\u001b[0m beamformer_output \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(beamformer_output)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "run_comparison(parameter_to_compare, algorithms_to_compare, nb_iterations, nbSources, nbSensors, theta, nbTimePoints, snr, correlation, var_ratio, perturbation_parameter_sd, two_symmetrical_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMusic_subregions_1 : [-19.  15.] DeepMusic_subregions_2 :  [-31.  20.]\n"
     ]
    }
   ],
   "source": [
    "from Signal_generator.generate_signal import generate_X_matrix\n",
    "\n",
    "nbSources = 2 # Nombre de sources\n",
    "nbSensors = 9 # Nombre de capteurs\n",
    "nbTimePoints = 100 # Nombre de points temporels\n",
    "signal_noise_ratio = 10 # Rapport signal sur bruit en décibels. Si 'False', cela revient à une absence totale de bruit.\n",
    "theta1 = -20 # Angle entre la perpendiculaire à la ligne de capteurs, et la source 1\n",
    "theta2 = 20 # Angle entre la perpendiculaire à la ligne de capteurs, et la source 2\n",
    "var_ratio = [1] # Liste qui donne le rapport entre la variance du signal 1 et celui des autres sources (ex: [2, 3] signifie que la source 2 a une variance 2 fois plus grande que la source 1, et la source 3 a une variance 3 fois plus grande que la source 1)\n",
    "correlation_List = [0] # Liste des corrélations. Il y a une corrélation nécéssaire pour chaque paire distincte de sources différentes: 0 pour 1 source, 1 pour 2 sources, 3 pour 3 sources, 6 pour 4 sources etc...\n",
    "# Ordre de remplisage de la correlation_List: de gauche à droite et ligne par ligne, moitié haut-droite de la matrice uniquement, puis symétrie de ces valeurs pour la moitié bas-gauche\n",
    "perturbation_parameter_sd = 0 # Écart-type de la distribution normale qui génère les erreurs de calibration des capteurs\n",
    "\n",
    "thetaList = [theta1, theta2]\n",
    "X = generate_X_matrix(nbSources, nbSensors, nbTimePoints, thetaList, var_ratio, correlation_List, signal_noise_ratio, perturbation_parameter_sd)\n",
    "print(\"DeepMusic_subregions_1 :\", deep_music_pred_1(X, nbSensors, nbSources), \"DeepMusic_subregions_2 : \", deep_music_pred_2(X, nbSensors, nbSources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMusic_subregions_1 : [-46.  15.] DeepMusic_subregions_2 :  [-15.  15.]\n"
     ]
    }
   ],
   "source": [
    "theta1 = -30 # Angle entre la perpendiculaire à la ligne de capteurs, et la source 1\n",
    "theta2 = 33 # Angle entre la perpendiculaire à la ligne de capteurs, et la source 2\n",
    "thetaList = [theta1, theta2]\n",
    "X = generate_X_matrix(nbSources, nbSensors, nbTimePoints, thetaList, var_ratio, correlation_List, signal_noise_ratio, perturbation_parameter_sd)\n",
    "print(\"DeepMusic_subregions_1 :\", deep_music_pred_1(X, nbSensors, nbSources), \"DeepMusic_subregions_2 : \", deep_music_pred_2(X, nbSensors, nbSources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-38.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-38.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-12.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [ 4. 54.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  55.] DeepMusic_subregions_2 :  [-15.  15.]\n",
      "DeepMusic_subregions_1 : [-36.  54.] DeepMusic_subregions_2 :  [-15.  15.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(\"DeepMusic_subregions_1 :\", deep_music_pred_1(generate_X_matrix(nbSources, nbSensors, nbTimePoints, thetaList, var_ratio, correlation_List, signal_noise_ratio, perturbation_parameter_sd), nbSensors, nbSources), \"DeepMusic_subregions_2 : \", deep_music_pred_2(generate_X_matrix(nbSources, nbSensors, nbTimePoints, thetaList, var_ratio, correlation_List, signal_noise_ratio, perturbation_parameter_sd), nbSensors, nbSources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
