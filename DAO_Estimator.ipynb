{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Signal_generator.generate_signal import generate_X_matrix\n",
    "from Algorithmes.beamforming import beamforming_method\n",
    "from Signal_generator.generate_signal import generate_A_matrix\n",
    "from Signal_generator.generate_signal import generate_noise\n",
    "from Signal_generator.generate_signal import generate_R_hat\n",
    "from Signal_generator.generate_signal import generate_R_hat_with_phase\n",
    "from Signal_generator.generate_signal import generate_R_hat_with_phase_complex\n",
    "from Algorithmes.music import music_method\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import itertools\n",
    "# Définition des paramètres du problème\n",
    "\n",
    "nbSources = 2 # Nombre de sources  dans l'article ils fixent le nombre de sources à 2\n",
    "nbSensors = 9 # Nombre de capteurs\n",
    "nbTimePoints = 100 # Nombre de points temporels\n",
    "var_ratio_1=1 # Rapport d intensité entre les deux signaux\n",
    "\n",
    "\n",
    "\n",
    "#Pour l'échantillon d'entraînements ils utilisent une observation de chaque combinaison d'angles d'arrivée possible pour chaque SNR level, dans le cas d'une seule source, 2G+1 observations\n",
    "#SNR ratio dans le papier est seulement low, de -20 à 0 DB avec un pas de 5\n",
    "#Petite note : si on fait un modèle pour chaque SNR level, les performances augmentent légérements, mais cela ne vaut pas l'effort effectué\n",
    "signal_noise_ratio = [-20,-15,-10,-5,0] # Rapport signal sur bruit en décibels. Si 'False', cela revient à une absence totale de bruit.\n",
    "L = nbSensors\n",
    "T = 100\n",
    "correlation_List = [0] # Liste des corrélations. Il y a une corrélation nécéssaire pour chaque paire distincte de sources différentes: 0 pour 1 source, 1 pour 2 sources, 3 pour 3 sources, 6 pour 4 sources etc...\n",
    "# Ordre de remplisage de la correlation_List: de gauche à droite et ligne par ligne, moitié haut-droite de la matrice uniquement, puis symétrie de ces valeurs pour la moitié bas-gauche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def angles_to_binary_vector(thetaList, G):\n",
    "    \"\"\"\n",
    "    Convertit une liste d'angles en un vecteur binaire représentant la présence/absence d'angles dans la grille G.\n",
    "    \n",
    "    Args:\n",
    "    - thetaList: Liste des angles d'arrivée pour un échantillon.\n",
    "    - G: Grille des angles possibles.\n",
    "    \n",
    "    Returns:\n",
    "    - Vecteur binaire des labels Z.\n",
    "    \"\"\"\n",
    "    Z = np.zeros(len(G), dtype=int)\n",
    "    for theta in thetaList:\n",
    "        index = np.searchsorted(G, theta)  # Trouver l'indice de l'angle dans la grille\n",
    "        if 0 <= index < len(G):  # S'assurer que l'indice est dans la plage valide\n",
    "            Z[index] = 1\n",
    "    return Z\n",
    "\n",
    "\n",
    "def generate_combinations(phi_max, rho, nb_sources):\n",
    "    \"\"\"Cette fonction sert à générer toutes les combinaisons d'angles possibles dans la plage de résolution\"\"\"\n",
    "    # Générer une plage de valeurs possibles pour les signaux\n",
    "    values = list(range(-int(phi_max / rho), int(phi_max / rho) + 1))\n",
    "    \n",
    "    # Générer toutes les combinaisons possibles de signaux\n",
    "    all_combinations = list(itertools.product(values, repeat=nb_sources))\n",
    "    \n",
    "    # Supprimer les combinaisons où l'ordre ne compte pas\n",
    "    unique_combinations = set()\n",
    "    for combination in all_combinations:\n",
    "        sorted_combination = tuple(sorted(combination))\n",
    "        unique_combinations.add(sorted_combination)\n",
    "    unique_combinations=list(unique_combinations)\n",
    "        \n",
    "    #supprimer les combinaisons ou deux signaux ont la même valeur\n",
    "    combination_a_supp=[]\n",
    "    for combination in unique_combinations:\n",
    "        presents=[]\n",
    "        for element in combination:\n",
    "            if element not in presents:\n",
    "                presents.append(element)\n",
    "            else:\n",
    "                combination_a_supp.append(combination)\n",
    "    for combination in combination_a_supp:\n",
    "        unique_combinations.remove(combination)\n",
    "                \n",
    "        \n",
    "    \n",
    "    return unique_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Paramètres pour la simulation\n",
    "\n",
    "num_samples = 1000  # Nombre d'échantillons à générer #16290\n",
    "L = nbSensors  # Nombre de capteurs\n",
    "T = 100  # Nombre de points temporels\n",
    "phi_max = 45  # φmax\n",
    "rho = 1 # Résolution\n",
    "\n",
    "# Grille des angles\n",
    "G = np.arange(-phi_max, phi_max + rho, rho)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération du set de donnée qui comprend toutes les combinaisons possibles de tous les angles de notre grille,\n",
    "# avec tout les ratio signaux sur bruit, toutes les corrélations et toutes les calibrations\n",
    "\n",
    "def generation_donnees( phi_max : int, rho : float, SNR : list, correlation : list, calibration ):\n",
    "    angles_combination=generate_combinations(phi_max,rho,nbSources)\n",
    "    num_samples=len(angles_combination)*len(SNR)\n",
    "    \n",
    "    # Initialisation des tableaux pour les données\n",
    "    all_R_hat_with_phase = np.zeros((num_samples, 3, L, L))\n",
    "    all_Z = np.zeros((num_samples, len(G)))\n",
    "    \n",
    "    #On parcourt les différents SNR possibles\n",
    "    i=0\n",
    "    for snr in SNR:\n",
    "        # Générer les angles d'arrivée et la matrice X comme décrit précédemment\n",
    "        for angles in angles_combination:\n",
    "            thetaList=list(angles)\n",
    "            var_ratio = [var_ratio_1]\n",
    "            X = generate_X_matrix(nbSources, L, T, thetaList, var_ratio, correlation_List, snr, perturbation_parameter_sd=0)# fonction pour générer X, ajoutez les paramètres nécessaires\n",
    "            \n",
    "            # Calculer R_hat étendue avec phase\n",
    "            R_hat_with_phase = generate_R_hat_with_phase(X)\n",
    "            all_R_hat_with_phase[i] = R_hat_with_phase\n",
    "            \n",
    "            # Calculer les labels Z pour les angles générés\n",
    "            all_Z[i] = angles_to_binary_vector(thetaList, G)\n",
    "            i+=1\n",
    "    return all_R_hat_with_phase, all_Z\n",
    "\n",
    "\n",
    "all_R_hat_with_phase, all_Z=generation_donnees(phi_max, rho, signal_noise_ratio,correlation_List,[])\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Génération des données\n",
    "\n",
    "# Initialisation des tableaux pour les données\n",
    "all_R_hat_with_phase = np.zeros((num_samples, 3, L, L))\n",
    "all_Z = np.zeros((num_samples, len(G)))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Générer les angles d'arrivée et la matrice X comme décrit précédemment\n",
    "    thetaList = [np.random.uniform(-phi_max, phi_max) for _ in range(nbSources)]  # Exemple de génération d'angles\n",
    "    var_ratio = [var_ratio_1]\n",
    "    X = generate_X_matrix(nbSources, L, T, thetaList, var_ratio, correlation_List, 0, perturbation_parameter_sd=0)  # Votre fonction pour générer X, ajoutez les paramètres nécessaires\n",
    "    \n",
    "    # Calculer R_hat étendue avec phase\n",
    "    R_hat_with_phase = generate_R_hat_with_phase(X)\n",
    "    all_R_hat_with_phase[i] = R_hat_with_phase\n",
    "    \n",
    "    # Calculer les labels Z pour les angles générés\n",
    "    all_Z[i] = angles_to_binary_vector(thetaList, G)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Z_train, Z_test = train_test_split(all_R_hat_with_phase, all_Z, test_size=0.1 , random_state=42)#test_size=0.1 selon papier\n",
    "\n",
    "#Conversion des données en tenseur\n",
    "X_train = torch.from_numpy(X_train).to(dtype=torch.float32)\n",
    "X_test = torch.from_numpy(X_test).to(dtype=torch.float32)\n",
    "Z_train = torch.from_numpy(Z_train).to(dtype=torch.float32)\n",
    "Z_test = torch.from_numpy(Z_test).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Création du dataset\n",
    "\n",
    "class DOADataset(Dataset):\n",
    "    def __init__(self, covariance_matrices, doa_labels):\n",
    "        \"\"\"\n",
    "        Initialisation du Dataset DOA.\n",
    "\n",
    "        Args:\n",
    "        - covariance_matrices (np.ndarray): Matrices de covariance étendues avec la phase.\n",
    "          Forme attendue: (nb_samples, nbSensors, nbSensors, 3) pour les parties réelle, imaginaire, et la phase.\n",
    "        - doa_labels (np.ndarray): Labels DOA sous forme de vecteurs binaires.\n",
    "          Forme attendue: (nb_samples, nb_labels), où nb_labels est le nombre de points dans la grille DOA.\n",
    "        \"\"\"\n",
    "        self.covariance_matrices = covariance_matrices\n",
    "        self.doa_labels = doa_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.covariance_matrices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      cov_matrix = self.covariance_matrices[idx]\n",
    "      angle = self.doa_labels[idx]\n",
    "\n",
    "      # Vérification si cov_matrix est déjà un ndarray, sinon le convertir\n",
    "      if not isinstance(cov_matrix, np.ndarray):\n",
    "          cov_matrix = cov_matrix.numpy()  # Convertir en tableau NumPy si ce n'est pas déjà fait\n",
    "    \n",
    "      cov_matrix = torch.from_numpy(cov_matrix).float()\n",
    "      angle = torch.tensor(angle).float()\n",
    "\n",
    "      return cov_matrix, angle\n",
    "\n",
    "\n",
    "# Exemple d'utilisation (remplacez R_hat_train/test et angles_train/test par vos données réelles)\n",
    "train_dataset = DOADataset(X_train, Z_train)\n",
    "test_dataset = DOADataset(X_test, Z_test)\n",
    "\n",
    "# Création des DataLoaders pour l'entraînement et le test, batch_size en fonction de l'article\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#Définition du device\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOACNN(\n",
      "  (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv7): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv10): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (relu6): ReLU()\n",
      "  (relu9): ReLU()\n",
      "  (relu12): ReLU()\n",
      "  (relu15): ReLU()\n",
      "  (relu18): ReLU()\n",
      "  (relu21): ReLU()\n",
      "  (dropout16): Dropout(p=0.2, inplace=False)\n",
      "  (dropout19): Dropout(p=0.2, inplace=False)\n",
      "  (dropout22): Dropout(p=0.2, inplace=False)\n",
      "  (flatten13): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc14): Linear(in_features=256, out_features=4096, bias=True)\n",
      "  (fc17): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (fc20): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc23): Linear(in_features=1024, out_features=91, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Définition du modèle\n",
    "nombre_de_classe=phi_max*2/rho+1\n",
    "nombre_de_classe=int(nombre_de_classe)\n",
    "class DOACNN(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_classes=nombre_de_classe):\n",
    "        super(DOACNN, self).__init__()\n",
    "\n",
    "        # Couches convolutionnelles 2D\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=2, stride=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=2, stride=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=2, stride=1)\n",
    "\n",
    "        # Normalisation de taille 256\n",
    "        self.norm2 = nn.BatchNorm2d(256)\n",
    "        self.norm5 = nn.BatchNorm2d(256)\n",
    "        self.norm8 = nn.BatchNorm2d(256)\n",
    "        self.norm11 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.relu3 =  nn.ReLU()\n",
    "        self.relu6 =  nn.ReLU()\n",
    "        self.relu9 =  nn.ReLU()\n",
    "        self.relu12 =  nn.ReLU()\n",
    "        self.relu15 =  nn.ReLU()\n",
    "        self.relu18 =  nn.ReLU()\n",
    "        self.relu21 =  nn.ReLU()\n",
    "\n",
    "        # Couches Dropout\n",
    "        self.dropout16 = nn.Dropout(0.2)\n",
    "        self.dropout19 = nn.Dropout(0.2)\n",
    "        self.dropout22 = nn.Dropout(0.2)\n",
    "\n",
    "        # Couche d'aplatissement\n",
    "        self.flatten13 = nn.Flatten()\n",
    "\n",
    "        # Couches entièrement connectées (FC)\n",
    "        self.fc14 = nn.Linear(in_features=256 , out_features=4096)\n",
    "        self.fc17 = nn.Linear(in_features=4096, out_features=2048)\n",
    "        self.fc20 = nn.Linear(in_features=2048, out_features=1024)\n",
    "        self.fc23 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "\n",
    "        # Couche de sortie Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm5(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.norm8(x)\n",
    "        x = self.relu9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.norm11(x)\n",
    "        x = self.relu12(x)\n",
    "        x = self.flatten13(x)\n",
    "        x = self.fc14(x)\n",
    "        x = self.relu15(x)\n",
    "        x = self.dropout16(x)\n",
    "        x = self.fc17(x)\n",
    "        x = self.relu18(x)\n",
    "        x = self.dropout19(x)\n",
    "        x = self.fc20(x)\n",
    "        x = self.relu21(x)\n",
    "        x = self.dropout22(x)\n",
    "        x = self.fc23(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Création d'une instance du modèle\n",
    "model = DOACNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Définition des hyperparamètres, choisis en fonction de l'article DOA estimator\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200 # à monter à 200 pour le training final selon l'article\n",
    "loss_fn = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boucle d'entraînement\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Display loss from time to time\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss: >7f}  [{current: >5d} / {size: >5d}]\")\n",
    "\n",
    "\n",
    "#Boucle de tests\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct, MSE = 0, 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            #print( f\"pred{pred.shape}\")\n",
    "            _, angles_predit = pred.topk(nbSources, dim=1)\n",
    "            _,angles_correct=top_values, top_indices = y.topk(nbSources, dim=1)\n",
    "            correct += (angles_predit==angles_correct).type(torch.float).sum().item()\n",
    "            #Calcul de la somme des carrés des différences entre prédictions et réalité pour la RMSE\n",
    "            #print( f\"MSE initial{MSE}\")\n",
    "            #print( f\"angles_predit{angles_predit}\")\n",
    "            taille_du_batch=pred.shape[0]\n",
    "            for element in range(taille_du_batch):\n",
    "                for angle in range(nbSources):\n",
    "                    MSE+=(angles_correct[element, angle]-angles_predit[element, angle])**2\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    MSE=MSE/(len(test_dataloader)*nbSources)\n",
    "    RMSE=math.sqrt(MSE)\n",
    "    print(f\"\\nTest set: \\n  Accuracy: {(100 * correct): >0.1f}%, Avg loss: {test_loss: >8f} \\n RMSE : {MSE: >8f}\")\n",
    "    return test_loss, correct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction d'entraînement et tests\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs, device):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        test_loop(test_dataloader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legra\\AppData\\Local\\Temp\\ipykernel_21088\\1026075063.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  angle = torch.tensor(angle).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.028235  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 26.0%, Avg loss: 0.112377 \n",
      " RMSE : 17779.750000\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.025215  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 34.0%, Avg loss: 0.115036 \n",
      " RMSE : 14707.500000\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.021329  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 32.0%, Avg loss: 0.112623 \n",
      " RMSE : 14557.750000\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.025743  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 30.0%, Avg loss: 0.112413 \n",
      " RMSE : 13098.125000\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.018212  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 36.0%, Avg loss: 0.112688 \n",
      " RMSE : 15084.500000\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.025506  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 33.0%, Avg loss: 0.111970 \n",
      " RMSE : 13086.875000\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.021971  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 33.0%, Avg loss: 0.113155 \n",
      " RMSE : 16418.625000\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.024072  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 31.0%, Avg loss: 0.113407 \n",
      " RMSE : 16486.500000\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.019356  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 35.0%, Avg loss: 0.113818 \n",
      " RMSE : 14186.375000\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.021829  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 29.0%, Avg loss: 0.115193 \n",
      " RMSE : 13861.125000\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.024332  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 27.0%, Avg loss: 0.113525 \n",
      " RMSE : 16399.375000\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.021018  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 30.0%, Avg loss: 0.112314 \n",
      " RMSE : 15524.750000\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.019670  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 28.0%, Avg loss: 0.112404 \n",
      " RMSE : 15831.250000\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.022070  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 32.0%, Avg loss: 0.110298 \n",
      " RMSE : 13842.875000\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.019861  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 30.0%, Avg loss: 0.112722 \n",
      " RMSE : 15219.125000\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.020472  [   32 /   900]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 31.0%, Avg loss: 0.112348 \n",
      " RMSE : 13846.125000\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.024460  [   32 /   900]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntraînement terminé!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[102], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     test_loop(test_dataloader, model, loss_fn, device)\n\u001b[0;32m      8\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[101], line 18\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, test_dataloader, loss_fn, optimizer, num_epochs, device)\n",
    "print('Entraînement terminé!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOACNN(\n",
      "  (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv7): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv10): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (relu6): ReLU()\n",
      "  (relu9): ReLU()\n",
      "  (relu12): ReLU()\n",
      "  (relu15): ReLU()\n",
      "  (relu18): ReLU()\n",
      "  (relu21): ReLU()\n",
      "  (dropout16): Dropout(p=0.2, inplace=False)\n",
      "  (dropout19): Dropout(p=0.2, inplace=False)\n",
      "  (dropout22): Dropout(p=0.2, inplace=False)\n",
      "  (flatten13): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc14): Linear(in_features=256, out_features=4096, bias=True)\n",
      "  (fc17): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (fc20): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc23): Linear(in_features=1024, out_features=91, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 4, 4]           7,168\n",
      "       BatchNorm2d-2            [-1, 256, 4, 4]             512\n",
      "              ReLU-3            [-1, 256, 4, 4]               0\n",
      "            Conv2d-4            [-1, 256, 3, 3]         262,400\n",
      "       BatchNorm2d-5            [-1, 256, 3, 3]             512\n",
      "              ReLU-6            [-1, 256, 3, 3]               0\n",
      "            Conv2d-7            [-1, 256, 2, 2]         262,400\n",
      "       BatchNorm2d-8            [-1, 256, 2, 2]             512\n",
      "              ReLU-9            [-1, 256, 2, 2]               0\n",
      "           Conv2d-10            [-1, 256, 1, 1]         262,400\n",
      "      BatchNorm2d-11            [-1, 256, 1, 1]             512\n",
      "             ReLU-12            [-1, 256, 1, 1]               0\n",
      "          Flatten-13                  [-1, 256]               0\n",
      "           Linear-14                 [-1, 4096]       1,052,672\n",
      "             ReLU-15                 [-1, 4096]               0\n",
      "          Dropout-16                 [-1, 4096]               0\n",
      "           Linear-17                 [-1, 2048]       8,390,656\n",
      "             ReLU-18                 [-1, 2048]               0\n",
      "          Dropout-19                 [-1, 2048]               0\n",
      "           Linear-20                 [-1, 1024]       2,098,176\n",
      "             ReLU-21                 [-1, 1024]               0\n",
      "          Dropout-22                 [-1, 1024]               0\n",
      "           Linear-23                   [-1, 91]          93,275\n",
      "          Sigmoid-24                   [-1, 91]               0\n",
      "================================================================\n",
      "Total params: 12,431,195\n",
      "Trainable params: 12,431,195\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.34\n",
      "Params size (MB): 47.42\n",
      "Estimated Total Size (MB): 47.77\n",
      "----------------------------------------------------------------\n",
      "Layer: conv1.weight | Size: torch.Size([256, 3, 3, 3])\n",
      "Layer: conv1.bias | Size: torch.Size([256])\n",
      "Layer: conv4.weight | Size: torch.Size([256, 256, 2, 2])\n",
      "Layer: conv4.bias | Size: torch.Size([256])\n",
      "Layer: conv7.weight | Size: torch.Size([256, 256, 2, 2])\n",
      "Layer: conv7.bias | Size: torch.Size([256])\n",
      "Layer: conv10.weight | Size: torch.Size([256, 256, 2, 2])\n",
      "Layer: conv10.bias | Size: torch.Size([256])\n",
      "Layer: norm2.weight | Size: torch.Size([256])\n",
      "Layer: norm2.bias | Size: torch.Size([256])\n",
      "Layer: norm5.weight | Size: torch.Size([256])\n",
      "Layer: norm5.bias | Size: torch.Size([256])\n",
      "Layer: norm8.weight | Size: torch.Size([256])\n",
      "Layer: norm8.bias | Size: torch.Size([256])\n",
      "Layer: norm11.weight | Size: torch.Size([256])\n",
      "Layer: norm11.bias | Size: torch.Size([256])\n",
      "Layer: fc14.weight | Size: torch.Size([4096, 256])\n",
      "Layer: fc14.bias | Size: torch.Size([4096])\n",
      "Layer: fc17.weight | Size: torch.Size([2048, 4096])\n",
      "Layer: fc17.bias | Size: torch.Size([2048])\n",
      "Layer: fc20.weight | Size: torch.Size([1024, 2048])\n",
      "Layer: fc20.bias | Size: torch.Size([1024])\n",
      "Layer: fc23.weight | Size: torch.Size([91, 1024])\n",
      "Layer: fc23.bias | Size: torch.Size([91])\n"
     ]
    }
   ],
   "source": [
    "#Quelques informations sur notre modèle : \n",
    "\n",
    "print(model)\n",
    "\n",
    "summary(model, input_size=(3, nbSensors, nbSensors))\n",
    "\n",
    "#Poids des paramètres :\n",
    "\n",
    "\n",
    "# Initialiser le compteur de paramètres\n",
    "total_params = 0\n",
    "\n",
    "# Boucle à travers les paramètres de votre modèle\n",
    "for param in model.parameters():\n",
    "    # Vérifier si le paramètre nécessite un gradient (c'est-à-dire s'il est entraînable)\n",
    "    if param.requires_grad:\n",
    "        # Compter le nombre de valeurs dans le paramètre\n",
    "        total_params += param.numel()\n",
    "\n",
    "# Boucle à travers les paramètres de votre modèle\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPuis voici la marche à suivre pour charger le modèle entrainé dans un nouveau module python\\n\\nimport torch\\nimport torch.nn as nn\\n\\n# Définir la classe de votre modèle (avec la même architecture que celle utilisée pour l'entraînement)\\nclass VotreModele(nn.Module):\\n    def __init__(self, ...):  # Définir les mêmes couches que celles utilisées pour l'entraînement\\n        super(VotreModele, self).__init__()\\n        ...\\n\\n# Créer une instance de votre modèle\\nmodel = VotreModele(...)\\n\\n# Charger les poids du modèle entraîné\\nmodel.load_state_dict(torch.load('chemin/vers/votre/modele.pth'))\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sauvegarder le modèle pour l'utiliser dans un autre module python:\n",
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\legra\\\\Documents pc\\\\ENSAI\\\\3A\\\\PFE\\\\model.pth')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Puis voici la marche à suivre pour charger le modèle entrainé dans un nouveau module python\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Définir la classe de votre modèle (avec la même architecture que celle utilisée pour l'entraînement)\n",
    "class VotreModele(nn.Module):\n",
    "    def __init__(self, ...):  # Définir les mêmes couches que celles utilisées pour l'entraînement\n",
    "        super(VotreModele, self).__init__()\n",
    "        ...\n",
    "\n",
    "# Créer une instance de votre modèle\n",
    "model = VotreModele(...)\n",
    "\n",
    "# Charger les poids du modèle entraîné\n",
    "model.load_state_dict(torch.load('chemin/vers/votre/modele.pth'))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.63159732e+11]]\n",
      "[[4.82987448e+11]]\n",
      "[[1.67230587e+11]]\n",
      "[[3.19478484e+11]]\n",
      "[[9.74600785e+11]]\n",
      "[[3.82785229e+10]]\n",
      "[[7.99166865e+11]]\n",
      "[[1.50108477e+11]]\n",
      "[[1.58339496e+11]]\n",
      "[[4.30877578e+10]]\n",
      "[[5.93102752e+11]]\n",
      "[[4.56835864e+11]]\n",
      "[[6.64769573e+11]]\n",
      "[[3.71191579e+11]]\n",
      "[[4.12925805e+11]]\n",
      "[[6.30419554e+11]]\n",
      "[[7.09760851e+11]]\n",
      "[[1.11243105e+11]]\n",
      "[[2.97458196e+11]]\n",
      "[[9.73690689e+11]]\n",
      "[[7.8889563e+11]]\n",
      "[[8.95380264e+11]]\n",
      "[[6.43780185e+11]]\n",
      "[[7.4314307e+11]]\n",
      "[[3.77719973e+11]]\n",
      "[[7.74742734e+11]]\n",
      "[[3.00329296e+11]]\n",
      "[[8.97970865e+11]]\n",
      "[[2.48066757e+10]]\n",
      "[[3.02099447e+11]]\n",
      "[[2.79137084e+11]]\n",
      "[[9.68363226e+11]]\n",
      "[[3.03763758e+10]]\n",
      "[[6.48619996e+11]]\n",
      "[[1.50234373e+11]]\n",
      "[[8.45255006e+10]]\n",
      "[[6.43650609e+11]]\n",
      "[[9.01890732e+10]]\n",
      "[[7.841603e+11]]\n",
      "[[4.54706853e+11]]\n",
      "[[7.53903969e+11]]\n",
      "[[2.57788275e+11]]\n",
      "[[4.6995895e+11]]\n",
      "[[4.84678618e+11]]\n",
      "[[4.73820299e+11]]\n",
      "[[5.89877483e+11]]\n",
      "[[3.20940418e+11]]\n",
      "[[2.43926637e+11]]\n",
      "[[1.01176451e+10]]\n",
      "[[6.84743916e+11]]\n",
      "[[1.58985186e+11]]\n",
      "[[2.36427574e+11]]\n",
      "[[9.65711675e+11]]\n",
      "[[3.51745451e+11]]\n",
      "[[6.21791836e+11]]\n",
      "[[7.15219581e+11]]\n",
      "[[5.65331294e+11]]\n",
      "[[7.52601426e+11]]\n",
      "[[7.58694077e+11]]\n",
      "[[8.31082942e+11]]\n",
      "[[4.51819561e+11]]\n",
      "[[9.59481344e+11]]\n",
      "[[7.59327735e+11]]\n",
      "[[3.54032623e+11]]\n",
      "[[1.40958708e+11]]\n",
      "[[3.55544327e+11]]\n",
      "[[2.42622567e+11]]\n",
      "[[6.2508251e+11]]\n",
      "[[7.86993402e+10]]\n",
      "[[1.28101559e+10]]\n",
      "[[2.23134679e+11]]\n",
      "[[3.82382411e+11]]\n",
      "[[2.39765679e+11]]\n",
      "[[1.44316545e+11]]\n",
      "[[3.85741328e+11]]\n",
      "[[4.93039447e+11]]\n",
      "[[7.27208414e+11]]\n",
      "[[5.25936859e+11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legra\\AppData\\Local\\Temp\\ipykernel_25528\\2233590894.py:15: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  all_R_hat_with_phase_complex[i] = R_hat_with_phase_complex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.06882189e+11]]\n",
      "[[6.17359942e+11]]\n",
      "[[6.02292975e+11]]\n",
      "[[7.84722122e+11]]\n",
      "[[5.77885756e+11]]\n",
      "[[3.3129245e+11]]\n",
      "[[1.83134671e+11]]\n",
      "[[5.96001252e+10]]\n",
      "[[7.45314371e+11]]\n",
      "[[3.02588074e+11]]\n",
      "[[4.00734176e+10]]\n",
      "[[2.59747991e+11]]\n",
      "[[5.73970082e+11]]\n",
      "[[8.41225473e+11]]\n",
      "[[9.55571918e+11]]\n",
      "[[8.5715119e+10]]\n",
      "[[4.63253281e+11]]\n",
      "[[3.39329667e+11]]\n",
      "[[3.17223883e+11]]\n",
      "[[3.24261366e+11]]\n",
      "[[7.98547375e+11]]\n",
      "[[8.16790022e+11]]\n",
      "[[3.26666472e+11]]\n",
      "[[6.33811868e+11]]\n",
      "[[2.43930298e+11]]\n",
      "[[6.34906094e+11]]\n",
      "[[1.45948221e+10]]\n",
      "[[7.07359304e+11]]\n",
      "[[9.81885007e+11]]\n",
      "[[1.78308994e+11]]\n",
      "[[4.05059453e+10]]\n",
      "[[3.25968689e+11]]\n",
      "[[5.74215033e+11]]\n",
      "[[5.17896082e+11]]\n",
      "[[1.78358424e+11]]\n",
      "[[8.56209372e+11]]\n",
      "[[5.27873433e+11]]\n",
      "[[6.32391304e+10]]\n",
      "[[2.76801507e+11]]\n",
      "[[9.71874422e+11]]\n",
      "[[6.50417341e+11]]\n",
      "[[3.54440503e+11]]\n",
      "[[6.26901896e+11]]\n",
      "[[4.9910087e+10]]\n",
      "[[9.62816894e+09]]\n",
      "[[6.88493476e+11]]\n",
      "[[1.840195e+10]]\n",
      "[[4.90175967e+11]]\n",
      "[[6.56781644e+11]]\n",
      "[[8.61733108e+11]]\n",
      "[[7.22717763e+11]]\n",
      "[[3.79571138e+11]]\n",
      "[[2.51309712e+11]]\n",
      "[[2.49507254e+10]]\n",
      "[[7.76487872e+11]]\n",
      "[[6.94503097e+11]]\n",
      "[[7.43096266e+11]]\n",
      "[[1.27318315e+11]]\n",
      "[[3.76408569e+11]]\n",
      "[[2.71384247e+11]]\n",
      "[[7.68575334e+11]]\n",
      "[[1.74939092e+11]]\n",
      "[[5.63044264e+11]]\n",
      "[[2.15294087e+11]]\n",
      "[[8.3911711e+09]]\n",
      "[[5.4089188e+11]]\n",
      "[[6.01462615e+11]]\n",
      "[[9.2696332e+11]]\n",
      "[[4.59220537e+10]]\n",
      "[[8.98763133e+11]]\n",
      "[[4.54612501e+11]]\n",
      "[[2.21709901e+11]]\n",
      "[[9.83837549e+10]]\n",
      "[[2.61085969e+11]]\n",
      "[[4.03404193e+11]]\n",
      "[[2.52492081e+11]]\n",
      "[[3.74895102e+11]]\n",
      "[[1.81961376e+10]]\n",
      "[[4.65881354e+11]]\n",
      "[[8.5821105e+11]]\n",
      "[[7.28085807e+11]]\n",
      "[[5.98115943e+11]]\n",
      "[[9.73146792e+11]]\n",
      "[[9.60381029e+11]]\n",
      "[[3.79217195e+11]]\n",
      "[[7.03072546e+11]]\n",
      "[[6.35979329e+11]]\n",
      "[[7.4451907e+11]]\n",
      "[[5.32231959e+11]]\n",
      "[[9.49870727e+11]]\n",
      "[[9.51646259e+11]]\n",
      "[[6.35345415e+11]]\n",
      "[[9.14008999e+11]]\n",
      "[[7.45653298e+11]]\n",
      "[[4.76819757e+10]]\n",
      "[[7.53908467e+11]]\n",
      "[[9.86854886e+11]]\n",
      "[[4.04744821e+10]]\n",
      "[[2.78607006e+11]]\n",
      "[[7.11997579e+10]]\n",
      "[[7.46756693e+11]]\n",
      "[[7.50518455e+11]]\n",
      "[[5.02633135e+11]]\n",
      "[[6.2726873e+10]]\n",
      "[[3.56560684e+11]]\n",
      "[[4.53902983e+11]]\n",
      "[[8.79755848e+11]]\n",
      "[[2.93678812e+11]]\n",
      "[[3.03341935e+11]]\n",
      "[[2.11699099e+11]]\n",
      "[[7.40165206e+10]]\n",
      "[[5.20173768e+11]]\n",
      "[[9.53568716e+11]]\n",
      "[[9.69305753e+11]]\n",
      "[[3.86698304e+11]]\n",
      "[[3.73563534e+11]]\n",
      "[[8.83088987e+11]]\n",
      "[[9.83364244e+10]]\n",
      "[[7.48383167e+10]]\n",
      "[[4.27734612e+11]]\n",
      "[[4.31794522e+11]]\n",
      "[[5.91396013e+11]]\n",
      "[[7.16808431e+11]]\n",
      "[[8.24726343e+11]]\n",
      "[[7.16259332e+11]]\n",
      "[[9.54042309e+11]]\n",
      "[[5.44984023e+11]]\n",
      "[[9.65581124e+11]]\n",
      "[[4.52774234e+11]]\n",
      "[[8.08386128e+11]]\n",
      "[[4.40874885e+11]]\n",
      "[[1.00375777e+11]]\n",
      "[[4.48587743e+11]]\n",
      "[[5.5958046e+11]]\n",
      "[[4.50871353e+11]]\n",
      "[[2.5688399e+11]]\n",
      "[[7.33837537e+10]]\n",
      "[[5.92558933e+11]]\n",
      "[[1.58991628e+11]]\n",
      "[[4.92228551e+11]]\n",
      "[[5.99404558e+11]]\n",
      "[[3.00893884e+10]]\n",
      "[[1.41767747e+11]]\n",
      "[[7.97967944e+11]]\n",
      "[[2.95150747e+11]]\n",
      "[[5.66778902e+11]]\n",
      "[[3.32786334e+11]]\n",
      "[[4.80711008e+11]]\n",
      "[[7.24230949e+11]]\n",
      "[[7.73040008e+11]]\n",
      "[[7.84882785e+11]]\n",
      "[[9.65994321e+11]]\n",
      "[[2.42272621e+11]]\n",
      "[[4.64997184e+11]]\n",
      "[[3.85756839e+11]]\n",
      "[[4.38718085e+11]]\n",
      "[[8.28301073e+11]]\n",
      "[[4.22547607e+11]]\n",
      "[[5.40285942e+11]]\n",
      "[[8.82989754e+11]]\n",
      "[[6.7965844e+11]]\n",
      "[[3.46845613e+11]]\n",
      "[[3.82281331e+11]]\n",
      "[[4.69022463e+10]]\n",
      "[[8.45204717e+11]]\n",
      "[[1.14488103e+11]]\n",
      "[[7.59861491e+11]]\n",
      "[[3.31645679e+11]]\n",
      "[[8.50938413e+11]]\n",
      "[[4.32235922e+11]]\n",
      "[[9.83491334e+11]]\n",
      "[[3.40817645e+11]]\n",
      "[[5.26116168e+11]]\n",
      "[[2.57830571e+11]]\n",
      "[[9.40142395e+11]]\n",
      "[[1.08558091e+11]]\n",
      "[[3.70038952e+11]]\n",
      "[[7.94038939e+10]]\n",
      "[[6.28378679e+11]]\n",
      "[[5.60510148e+11]]\n",
      "[[7.3086543e+11]]\n",
      "[[8.04620556e+11]]\n",
      "[[6.95624323e+11]]\n",
      "[[4.75793872e+11]]\n",
      "[[8.08496887e+11]]\n",
      "[[7.2951062e+11]]\n",
      "[[7.48827027e+11]]\n",
      "[[7.73123058e+11]]\n",
      "[[3.36237105e+11]]\n",
      "[[3.76433862e+10]]\n",
      "[[6.52504502e+11]]\n",
      "[[1.52076354e+11]]\n",
      "[[8.59140734e+11]]\n",
      "[[2.87074098e+11]]\n",
      "[[5.33178612e+11]]\n",
      "[[5.29355542e+11]]\n",
      "[[3.35202932e+11]]\n",
      "[[8.80984367e+11]]\n",
      "[[7.08561084e+11]]\n",
      "[[1.54404748e+11]]\n",
      "[[9.13380748e+11]]\n",
      "[[1.54800341e+11]]\n",
      "[[6.85825122e+11]]\n",
      "[[9.33418398e+11]]\n",
      "[[4.1196673e+11]]\n",
      "[[9.47072224e+11]]\n",
      "[[8.12785543e+11]]\n",
      "[[3.18541897e+11]]\n",
      "[[4.2338023e+10]]\n",
      "[[7.89801734e+11]]\n",
      "[[8.51082216e+11]]\n",
      "[[6.92089617e+11]]\n",
      "[[4.90046188e+11]]\n",
      "[[8.72050743e+11]]\n",
      "[[6.70229262e+11]]\n",
      "[[8.93047579e+11]]\n",
      "[[4.1780885e+11]]\n",
      "[[6.89594882e+11]]\n",
      "[[8.12990047e+11]]\n",
      "[[2.32689091e+11]]\n",
      "[[8.08265907e+11]]\n",
      "[[3.10914171e+11]]\n",
      "[[6.29557807e+11]]\n",
      "[[5.07871044e+11]]\n",
      "[[8.26387903e+11]]\n",
      "[[3.2599175e+10]]\n",
      "[[2.18780815e+11]]\n",
      "[[6.06299951e+11]]\n",
      "[[7.9911256e+11]]\n",
      "[[1.21536734e+11]]\n",
      "[[8.92465663e+11]]\n",
      "[[6.11734935e+10]]\n",
      "[[9.32567104e+11]]\n",
      "[[4.18158422e+11]]\n",
      "[[6.83095114e+11]]\n",
      "[[9.69349808e+11]]\n",
      "[[2.95095993e+11]]\n",
      "[[1.61263443e+11]]\n",
      "[[8.77088069e+11]]\n",
      "[[6.79274427e+09]]\n",
      "[[8.85541945e+11]]\n",
      "[[5.83490079e+11]]\n",
      "[[1.67531682e+11]]\n",
      "[[5.28083292e+11]]\n",
      "[[6.58459415e+10]]\n",
      "[[7.96153795e+11]]\n",
      "[[1.29255056e+11]]\n",
      "[[5.87750207e+11]]\n",
      "[[2.41900318e+11]]\n",
      "[[4.6986334e+11]]\n",
      "[[4.09887627e+10]]\n",
      "[[7.22510516e+11]]\n",
      "[[5.71455911e+11]]\n",
      "[[7.38502618e+11]]\n",
      "[[9.08787666e+10]]\n",
      "[[7.8830615e+10]]\n",
      "[[4.63732214e+11]]\n",
      "[[5.29719374e+11]]\n",
      "[[3.94855448e+11]]\n",
      "[[3.73196112e+11]]\n",
      "[[6.07111453e+10]]\n",
      "[[6.26466607e+11]]\n",
      "[[3.6029619e+11]]\n",
      "[[8.57057514e+10]]\n",
      "[[5.56487918e+11]]\n",
      "[[2.24740676e+11]]\n",
      "[[8.36896775e+11]]\n",
      "[[5.85622767e+11]]\n",
      "[[7.77774533e+10]]\n",
      "[[8.28580599e+11]]\n",
      "[[3.37492305e+11]]\n",
      "[[6.58930732e+11]]\n",
      "[[5.87128363e+11]]\n",
      "[[3.49251304e+11]]\n",
      "[[5.02293811e+11]]\n",
      "[[3.99503967e+11]]\n",
      "[[6.04783584e+11]]\n",
      "[[3.93932297e+11]]\n",
      "[[9.4965742e+11]]\n",
      "[[7.68108375e+11]]\n",
      "[[8.1852827e+09]]\n",
      "[[4.43759488e+11]]\n",
      "[[8.69653953e+11]]\n",
      "[[7.1076693e+11]]\n",
      "[[3.3578673e+11]]\n",
      "[[2.05227774e+11]]\n",
      "[[4.55263613e+11]]\n",
      "[[9.30491496e+11]]\n",
      "[[9.74415183e+11]]\n",
      "[[5.23624458e+11]]\n",
      "[[2.53508094e+11]]\n",
      "[[2.32390774e+11]]\n",
      "[[1.77932407e+11]]\n",
      "[[5.36653186e+10]]\n",
      "[[1.39207506e+11]]\n",
      "[[3.55054312e+11]]\n",
      "[[9.47586896e+11]]\n",
      "[[4.75419461e+11]]\n",
      "[[5.39989483e+11]]\n",
      "[[2.02720738e+11]]\n",
      "[[4.9176703e+11]]\n",
      "[[9.80228233e+11]]\n",
      "[[2.04029406e+11]]\n",
      "[[5.92777596e+11]]\n",
      "[[1.85080751e+11]]\n",
      "[[6.37733044e+11]]\n",
      "[[1.27382478e+11]]\n",
      "[[4.2014132e+11]]\n",
      "[[8.75276766e+11]]\n",
      "[[1.38098641e+11]]\n",
      "[[8.71106737e+11]]\n",
      "[[2.34220677e+11]]\n",
      "[[1.4277239e+11]]\n",
      "[[3.88375091e+11]]\n",
      "[[4.97932045e+11]]\n",
      "[[9.01371464e+11]]\n",
      "[[5.92420868e+11]]\n",
      "[[5.29203195e+11]]\n",
      "[[6.79665372e+11]]\n",
      "[[3.28483265e+11]]\n",
      "[[6.37072193e+11]]\n",
      "[[4.22433606e+11]]\n",
      "[[1.69187379e+11]]\n",
      "[[9.22522766e+11]]\n",
      "[[7.61026774e+11]]\n",
      "[[8.89470475e+11]]\n",
      "[[2.36316321e+11]]\n",
      "[[1.6431752e+11]]\n",
      "[[5.00418234e+11]]\n",
      "[[5.30778978e+11]]\n",
      "[[9.81943094e+11]]\n",
      "[[5.63285737e+11]]\n",
      "[[9.08937536e+11]]\n",
      "[[1.15503194e+11]]\n",
      "[[8.11535559e+11]]\n",
      "[[3.89532274e+11]]\n",
      "[[7.60532536e+11]]\n",
      "[[7.76608708e+10]]\n",
      "[[1.80285594e+11]]\n",
      "[[1.06773261e+11]]\n",
      "[[2.2559465e+11]]\n",
      "[[4.5473723e+11]]\n",
      "[[9.95292676e+11]]\n",
      "[[8.57975632e+11]]\n",
      "[[1.64478163e+11]]\n",
      "[[1.26583202e+11]]\n",
      "[[8.61232432e+11]]\n",
      "[[1.32054031e+11]]\n",
      "[[3.05662899e+11]]\n",
      "[[8.43836924e+09]]\n",
      "[[4.91206341e+11]]\n",
      "[[2.71493623e+11]]\n",
      "[[2.71142984e+11]]\n",
      "[[6.96725095e+11]]\n",
      "[[9.71170899e+11]]\n",
      "[[5.18772862e+11]]\n",
      "[[5.55326226e+11]]\n",
      "[[6.76009987e+11]]\n",
      "[[5.90447116e+11]]\n",
      "[[9.44528963e+11]]\n",
      "[[6.29009344e+11]]\n",
      "[[3.39727727e+11]]\n",
      "[[3.0790909e+11]]\n",
      "[[1.1896521e+11]]\n",
      "[[7.52214737e+11]]\n",
      "[[3.03837994e+11]]\n",
      "[[5.81742666e+11]]\n",
      "[[6.25621772e+11]]\n",
      "[[4.65880588e+11]]\n",
      "[[5.92707925e+11]]\n",
      "[[7.78647541e+10]]\n",
      "[[6.62627025e+11]]\n",
      "[[2.75750297e+11]]\n",
      "[[3.21085075e+11]]\n",
      "[[8.02121473e+11]]\n",
      "[[1.48785801e+10]]\n",
      "[[9.80977277e+11]]\n",
      "[[7.00775331e+11]]\n",
      "[[9.96300643e+11]]\n",
      "[[9.33485106e+11]]\n",
      "[[3.9250948e+11]]\n",
      "[[2.74371351e+11]]\n",
      "[[2.323109e+11]]\n",
      "[[5.82075057e+11]]\n",
      "[[2.88598715e+11]]\n",
      "[[5.08615125e+11]]\n",
      "[[7.82733023e+11]]\n",
      "[[9.88026801e+11]]\n",
      "[[8.62364076e+11]]\n",
      "[[4.99402241e+11]]\n",
      "[[1.1641782e+11]]\n",
      "[[5.43635068e+11]]\n",
      "[[9.82374061e+11]]\n",
      "[[3.66556299e+11]]\n",
      "[[1.24359228e+11]]\n",
      "[[1.4744626e+11]]\n",
      "[[1.93410118e+11]]\n",
      "[[9.4653565e+10]]\n",
      "[[7.65665946e+11]]\n",
      "[[2.80176481e+11]]\n",
      "[[5.10820697e+11]]\n",
      "[[9.31012231e+11]]\n",
      "[[3.23123017e+10]]\n",
      "[[1.0584076e+11]]\n",
      "[[5.73461992e+11]]\n",
      "[[8.0732574e+11]]\n",
      "[[2.87174961e+11]]\n",
      "[[5.65699575e+11]]\n",
      "[[7.24436285e+11]]\n",
      "[[8.58288852e+11]]\n",
      "[[7.68452e+11]]\n",
      "[[7.73619737e+11]]\n",
      "[[6.80934804e+10]]\n",
      "[[4.43948822e+11]]\n",
      "[[7.05378221e+11]]\n",
      "[[1.59825113e+11]]\n",
      "[[2.47775489e+11]]\n",
      "[[9.13684231e+11]]\n",
      "[[6.37133652e+11]]\n",
      "[[9.38443451e+11]]\n",
      "[[4.60231072e+11]]\n",
      "[[2.26487086e+11]]\n",
      "[[3.78734775e+11]]\n",
      "[[2.30494514e+11]]\n",
      "[[1.99824378e+11]]\n",
      "[[8.98444728e+11]]\n",
      "[[3.06835382e+10]]\n",
      "[[1.09453477e+11]]\n",
      "[[3.23678945e+10]]\n",
      "[[9.0790784e+11]]\n",
      "[[7.49888764e+11]]\n",
      "[[8.18947307e+11]]\n",
      "[[3.09724761e+11]]\n",
      "[[2.99802711e+11]]\n",
      "[[9.68768623e+11]]\n",
      "[[6.5270579e+11]]\n",
      "[[8.03771247e+11]]\n",
      "[[8.20467272e+11]]\n",
      "[[6.8037382e+11]]\n",
      "[[3.33585899e+11]]\n",
      "[[2.34850852e+11]]\n",
      "[[1.44837626e+11]]\n",
      "[[6.08813175e+11]]\n",
      "[[2.30646834e+11]]\n",
      "[[3.84746049e+11]]\n",
      "[[1.72056978e+10]]\n",
      "[[5.29511466e+11]]\n",
      "[[3.00757641e+11]]\n",
      "[[7.33568897e+11]]\n",
      "[[7.15791186e+11]]\n",
      "[[6.14767356e+11]]\n",
      "[[8.81301546e+11]]\n",
      "[[3.1629991e+11]]\n",
      "[[2.8348372e+11]]\n",
      "[[1.13188815e+10]]\n",
      "[[9.0276131e+11]]\n",
      "[[4.11485337e+11]]\n",
      "[[6.64931653e+11]]\n",
      "[[6.06200151e+11]]\n",
      "[[5.52274416e+11]]\n",
      "[[3.80605365e+11]]\n",
      "[[3.23124573e+11]]\n",
      "[[2.52483697e+11]]\n",
      "[[2.54877687e+11]]\n",
      "[[1.56131153e+11]]\n",
      "[[2.00692011e+11]]\n",
      "[[3.55814288e+11]]\n",
      "[[7.53818164e+11]]\n",
      "[[5.74074131e+11]]\n",
      "[[6.79504921e+11]]\n",
      "[[3.92603311e+11]]\n",
      "[[6.85833384e+11]]\n",
      "[[9.81473615e+11]]\n",
      "[[1.55897882e+11]]\n",
      "[[8.04851974e+11]]\n",
      "[[7.56411878e+11]]\n",
      "[[4.81323144e+11]]\n",
      "[[5.70556629e+11]]\n",
      "[[7.75594663e+10]]\n",
      "[[5.31011543e+11]]\n",
      "[[7.68785257e+11]]\n",
      "[[1.6269426e+11]]\n",
      "[[1.68839626e+11]]\n",
      "[[2.86272975e+11]]\n",
      "[[5.08792909e+11]]\n",
      "[[8.42363179e+11]]\n",
      "[[6.70837026e+11]]\n",
      "[[5.56284184e+11]]\n",
      "[[5.8426953e+11]]\n",
      "[[9.11607242e+11]]\n",
      "[[6.13609548e+11]]\n",
      "[[2.05617316e+11]]\n",
      "[[7.9549803e+11]]\n",
      "[[7.30415987e+11]]\n",
      "[[2.56431726e+11]]\n",
      "[[8.81040749e+11]]\n",
      "[[6.92675249e+11]]\n",
      "[[1.82656104e+11]]\n",
      "[[7.38019947e+10]]\n",
      "[[7.99761782e+11]]\n",
      "[[7.52584192e+11]]\n",
      "[[5.93561403e+11]]\n",
      "[[1.01153551e+11]]\n",
      "[[6.68483346e+11]]\n",
      "[[9.65416763e+11]]\n",
      "[[8.28399179e+11]]\n",
      "[[4.81683136e+10]]\n",
      "[[1.13947694e+11]]\n",
      "[[3.78446544e+11]]\n",
      "[[6.74333835e+11]]\n",
      "[[8.85430772e+11]]\n",
      "[[5.51048763e+11]]\n",
      "[[5.59563603e+11]]\n",
      "[[4.05246076e+11]]\n",
      "[[7.64195811e+11]]\n",
      "[[4.65896153e+10]]\n",
      "[[8.11624242e+11]]\n",
      "[[9.63393877e+11]]\n",
      "[[3.70999837e+11]]\n",
      "[[8.87876111e+11]]\n",
      "[[9.61682892e+11]]\n",
      "[[5.52883315e+11]]\n",
      "[[6.42139675e+11]]\n",
      "[[8.0682044e+11]]\n",
      "[[9.70050117e+11]]\n",
      "[[8.34666162e+11]]\n",
      "[[3.37839905e+11]]\n",
      "[[5.19577261e+11]]\n",
      "[[9.23861069e+11]]\n",
      "[[8.47570165e+11]]\n",
      "[[9.91133156e+11]]\n",
      "[[8.19016766e+11]]\n",
      "[[2.95028924e+11]]\n",
      "[[5.68342068e+11]]\n",
      "[[5.27775778e+11]]\n",
      "[[4.33986388e+11]]\n",
      "[[7.4965804e+11]]\n",
      "[[3.63359231e+11]]\n",
      "[[7.42337381e+11]]\n",
      "[[5.10803983e+11]]\n",
      "[[4.63080037e+11]]\n",
      "[[8.54407768e+11]]\n",
      "[[7.31851916e+11]]\n",
      "[[5.0214738e+09]]\n",
      "[[7.32320626e+11]]\n",
      "[[4.28322363e+11]]\n",
      "[[4.71497308e+11]]\n",
      "[[7.17433901e+11]]\n",
      "[[7.89781236e+11]]\n",
      "[[2.52364454e+11]]\n",
      "[[8.16620262e+11]]\n",
      "[[9.5254052e+10]]\n",
      "[[9.5473962e+11]]\n",
      "[[9.21672544e+11]]\n",
      "[[1.65607268e+11]]\n",
      "[[2.27630083e+11]]\n",
      "[[8.10942946e+11]]\n",
      "[[6.51214028e+10]]\n",
      "[[9.64989026e+10]]\n",
      "[[3.73451048e+10]]\n",
      "[[7.39751868e+11]]\n",
      "[[4.62904134e+11]]\n",
      "[[6.96681351e+11]]\n",
      "[[9.19190622e+11]]\n",
      "[[8.13991019e+10]]\n",
      "[[7.776053e+11]]\n",
      "[[3.80308497e+11]]\n",
      "[[1.16492772e+11]]\n",
      "[[8.35814003e+11]]\n",
      "[[1.6417944e+11]]\n",
      "[[7.54980057e+11]]\n",
      "[[4.32768178e+10]]\n",
      "[[5.02574653e+11]]\n",
      "[[4.14481108e+11]]\n",
      "[[9.20182531e+11]]\n",
      "[[1.73149278e+11]]\n",
      "[[1.23601773e+11]]\n",
      "[[5.45165128e+11]]\n",
      "[[6.61703824e+11]]\n",
      "[[2.14732443e+11]]\n",
      "[[6.46694128e+11]]\n",
      "[[5.92878723e+11]]\n",
      "[[8.22109809e+11]]\n",
      "[[4.60139606e+11]]\n",
      "[[1.07251564e+11]]\n",
      "[[5.04585654e+11]]\n",
      "[[5.14583164e+11]]\n",
      "[[9.78522214e+11]]\n",
      "[[2.95078752e+11]]\n",
      "[[5.01788821e+11]]\n",
      "[[9.94791704e+11]]\n",
      "[[9.03267792e+11]]\n",
      "[[9.13482879e+11]]\n",
      "[[7.93887723e+11]]\n",
      "[[4.11928166e+11]]\n",
      "[[6.5278394e+11]]\n",
      "[[3.72868912e+11]]\n",
      "[[9.18366366e+09]]\n",
      "[[1.7994802e+11]]\n",
      "[[7.66250636e+11]]\n",
      "[[8.624765e+11]]\n",
      "[[9.12237407e+11]]\n",
      "[[4.13021188e+11]]\n",
      "[[4.81530517e+11]]\n",
      "[[5.32411317e+11]]\n",
      "[[6.1132172e+11]]\n",
      "[[3.37154114e+09]]\n",
      "[[2.59296733e+11]]\n",
      "[[7.89191744e+11]]\n",
      "[[6.9402838e+11]]\n",
      "[[4.78068896e+11]]\n",
      "[[9.29127125e+10]]\n",
      "[[2.64817288e+11]]\n",
      "[[4.40537778e+11]]\n",
      "[[3.0189709e+10]]\n",
      "[[3.99072785e+11]]\n",
      "[[5.62471523e+11]]\n",
      "[[6.90037013e+11]]\n",
      "[[5.39630152e+10]]\n",
      "[[9.39929702e+11]]\n",
      "[[2.03413161e+11]]\n",
      "[[9.82965272e+11]]\n",
      "[[2.11612082e+11]]\n",
      "[[3.07658833e+11]]\n",
      "[[6.87126894e+11]]\n",
      "[[3.73901607e+11]]\n",
      "[[6.36988659e+11]]\n",
      "[[7.38406522e+11]]\n",
      "[[6.12809843e+11]]\n",
      "[[4.92195696e+11]]\n",
      "[[8.14801306e+11]]\n",
      "[[5.32892703e+11]]\n",
      "[[7.61376342e+11]]\n",
      "[[3.33844888e+11]]\n",
      "[[4.24546993e+11]]\n",
      "[[6.50953344e+10]]\n",
      "[[5.21239691e+11]]\n",
      "[[4.75539578e+11]]\n",
      "[[7.71921811e+11]]\n",
      "[[1.97364515e+11]]\n",
      "[[1.97931495e+11]]\n",
      "[[4.5757104e+11]]\n",
      "[[6.46991835e+11]]\n",
      "[[5.49497671e+11]]\n",
      "[[3.99121549e+11]]\n",
      "[[3.79062483e+11]]\n",
      "[[7.83851223e+11]]\n",
      "[[4.63418093e+11]]\n",
      "[[9.10513641e+11]]\n",
      "[[4.60627571e+10]]\n",
      "[[4.74141078e+11]]\n",
      "[[1.6418483e+11]]\n",
      "[[6.39578147e+11]]\n",
      "[[4.33052616e+11]]\n",
      "[[8.10512303e+11]]\n",
      "[[6.52635642e+11]]\n",
      "[[1.51939139e+11]]\n",
      "[[1.47911425e+11]]\n",
      "[[1.59337154e+11]]\n",
      "[[2.11001918e+11]]\n",
      "[[7.59822742e+11]]\n",
      "[[9.90235483e+11]]\n",
      "[[7.89700889e+11]]\n",
      "[[1.19087156e+11]]\n",
      "[[8.21831499e+10]]\n",
      "[[7.47987205e+11]]\n",
      "[[8.50853044e+11]]\n",
      "[[2.00304663e+10]]\n",
      "[[7.15713374e+11]]\n",
      "[[2.80152836e+11]]\n",
      "[[8.87739852e+11]]\n",
      "[[3.14038781e+10]]\n",
      "[[5.77092771e+11]]\n",
      "[[2.13312149e+11]]\n",
      "[[6.76333135e+11]]\n",
      "[[1.63230162e+11]]\n",
      "[[4.65033753e+11]]\n",
      "[[6.81229341e+11]]\n",
      "[[8.45936884e+11]]\n",
      "[[2.52290404e+11]]\n",
      "[[5.93804454e+11]]\n",
      "[[9.15022725e+11]]\n",
      "[[2.50487218e+11]]\n",
      "[[9.45614205e+10]]\n",
      "[[4.47421266e+11]]\n",
      "[[1.98348802e+11]]\n",
      "[[1.58265374e+11]]\n",
      "[[9.94833679e+11]]\n",
      "[[2.63723839e+11]]\n",
      "[[7.25662755e+11]]\n",
      "[[9.56070956e+11]]\n",
      "[[2.11539983e+11]]\n",
      "[[2.82950684e+11]]\n",
      "[[3.57890214e+11]]\n",
      "[[7.46222777e+11]]\n",
      "[[5.12108819e+11]]\n",
      "[[1.19500447e+11]]\n",
      "[[7.86641624e+11]]\n",
      "[[3.14534643e+11]]\n",
      "[[6.31205933e+11]]\n",
      "[[2.95652114e+11]]\n",
      "[[5.28650794e+11]]\n",
      "[[2.05718814e+11]]\n",
      "[[6.86355618e+11]]\n",
      "[[4.10833192e+11]]\n",
      "[[3.39039084e+11]]\n",
      "[[9.62842382e+11]]\n",
      "[[4.85473604e+11]]\n",
      "[[2.61141693e+11]]\n",
      "[[3.37809062e+11]]\n",
      "[[8.02301724e+11]]\n",
      "[[1.36993461e+11]]\n",
      "[[9.65650611e+11]]\n",
      "[[5.91694053e+11]]\n",
      "[[1.62353249e+11]]\n",
      "[[6.01973999e+11]]\n",
      "[[4.22429362e+11]]\n",
      "[[5.27955095e+11]]\n",
      "[[4.48550673e+11]]\n",
      "[[2.33709195e+11]]\n",
      "[[4.92404069e+11]]\n",
      "[[4.99796743e+11]]\n",
      "[[8.29811755e+11]]\n",
      "[[3.14115297e+10]]\n",
      "[[2.76531329e+11]]\n",
      "[[2.77777996e+10]]\n",
      "[[1.9163105e+10]]\n",
      "[[1.81990365e+11]]\n",
      "[[7.51042182e+10]]\n",
      "[[3.09193344e+11]]\n",
      "[[6.27142714e+11]]\n",
      "[[1.63115479e+11]]\n",
      "[[8.17487979e+11]]\n",
      "[[4.62378107e+11]]\n",
      "[[4.19309173e+11]]\n",
      "[[8.18494006e+11]]\n",
      "[[5.3962344e+10]]\n",
      "[[6.89012771e+11]]\n",
      "[[8.65291576e+11]]\n",
      "[[3.95290598e+11]]\n",
      "[[8.84261428e+11]]\n",
      "[[2.24191317e+11]]\n",
      "[[9.4519339e+10]]\n",
      "[[9.65036886e+10]]\n",
      "[[4.49786205e+11]]\n",
      "[[3.72415776e+11]]\n",
      "[[2.33751174e+11]]\n",
      "[[4.93293564e+11]]\n",
      "[[6.12058727e+11]]\n",
      "[[2.14127079e+11]]\n",
      "[[9.76179651e+11]]\n",
      "[[6.2509054e+11]]\n",
      "[[7.55631026e+11]]\n",
      "[[4.03870655e+11]]\n",
      "[[7.78031059e+11]]\n",
      "[[8.19622167e+10]]\n",
      "[[5.17980469e+11]]\n",
      "[[5.32141147e+11]]\n",
      "[[7.05149345e+11]]\n",
      "[[8.39001387e+11]]\n",
      "[[2.66827383e+11]]\n",
      "[[6.42422323e+11]]\n",
      "[[4.79851414e+11]]\n",
      "[[2.40640682e+11]]\n",
      "[[1.33959984e+11]]\n",
      "[[1.88043889e+11]]\n",
      "[[2.81355605e+10]]\n",
      "[[2.86138068e+11]]\n",
      "[[5.12004116e+11]]\n",
      "[[3.22077114e+11]]\n",
      "[[4.40482105e+11]]\n",
      "[[1.25712508e+11]]\n",
      "[[6.51513143e+11]]\n",
      "[[9.2033738e+09]]\n",
      "[[8.79331176e+11]]\n",
      "[[4.45340511e+11]]\n",
      "[[2.07498027e+11]]\n",
      "[[2.55278802e+11]]\n",
      "[[5.20550503e+11]]\n",
      "[[3.07043443e+11]]\n",
      "[[8.06618715e+10]]\n",
      "[[4.48733502e+11]]\n",
      "[[5.65863508e+10]]\n",
      "[[8.6472887e+11]]\n",
      "[[7.5828692e+11]]\n",
      "[[9.71905007e+11]]\n",
      "[[6.57996545e+11]]\n",
      "[[4.33134174e+11]]\n",
      "[[1.74456696e+11]]\n",
      "[[6.24843405e+11]]\n",
      "[[7.99929514e+10]]\n",
      "[[3.63578975e+10]]\n",
      "[[6.07781097e+11]]\n",
      "[[2.12909611e+11]]\n",
      "[[9.06299606e+11]]\n",
      "[[3.75359623e+11]]\n",
      "[[1.85681097e+11]]\n",
      "[[2.67458025e+11]]\n",
      "[[5.15726467e+11]]\n",
      "[[5.40824588e+11]]\n",
      "[[7.15363537e+11]]\n",
      "[[8.08526533e+11]]\n",
      "[[5.78282386e+11]]\n",
      "[[5.11290611e+11]]\n",
      "[[9.72028941e+11]]\n",
      "[[8.46569042e+11]]\n",
      "[[5.68151106e+11]]\n",
      "[[1.21884712e+11]]\n",
      "[[7.77238684e+11]]\n",
      "[[7.10373215e+11]]\n",
      "[[2.01992993e+11]]\n",
      "[[6.45820597e+11]]\n",
      "[[8.15714067e+10]]\n",
      "[[6.2746628e+10]]\n",
      "[[3.7440219e+11]]\n",
      "[[7.27239957e+11]]\n",
      "[[7.49872934e+11]]\n",
      "[[2.19341878e+11]]\n",
      "[[9.72965276e+11]]\n",
      "[[4.80593531e+11]]\n",
      "[[8.39313832e+11]]\n",
      "[[2.64902139e+11]]\n",
      "[[6.16910525e+11]]\n",
      "[[1.1925764e+11]]\n",
      "[[8.48002522e+11]]\n",
      "[[8.73603261e+11]]\n",
      "[[3.4646571e+11]]\n",
      "[[6.47502923e+11]]\n",
      "[[5.06127758e+11]]\n",
      "[[7.67920032e+11]]\n",
      "[[7.70477704e+11]]\n",
      "[[4.23086413e+10]]\n",
      "[[3.47742472e+11]]\n",
      "[[1.27997222e+11]]\n",
      "[[4.49548456e+11]]\n",
      "[[8.78555963e+11]]\n",
      "[[2.73177561e+11]]\n",
      "[[6.4812007e+11]]\n",
      "[[3.0469389e+11]]\n",
      "[[8.65726706e+11]]\n",
      "[[1.30404157e+11]]\n",
      "[[7.16967214e+11]]\n",
      "[[4.84553165e+11]]\n",
      "[[5.06232499e+11]]\n",
      "[[3.91346353e+11]]\n",
      "[[9.93319522e+11]]\n",
      "[[6.98779815e+11]]\n",
      "[[1.90529229e+11]]\n",
      "[[1.54381739e+11]]\n",
      "[[1.97203266e+11]]\n",
      "[[5.94124928e+11]]\n",
      "[[5.94749435e+11]]\n",
      "[[1.99216931e+11]]\n",
      "[[7.35612539e+11]]\n",
      "[[9.26153186e+11]]\n",
      "[[7.29957026e+10]]\n",
      "[[8.01678756e+11]]\n",
      "[[1.27372509e+11]]\n",
      "[[3.91153776e+11]]\n",
      "[[2.18469541e+11]]\n",
      "[[1.31437552e+11]]\n",
      "[[6.91364718e+10]]\n",
      "[[4.31892603e+11]]\n",
      "[[7.79226443e+11]]\n",
      "[[9.35389272e+11]]\n",
      "[[5.29715325e+11]]\n",
      "[[6.40638367e+11]]\n",
      "[[9.19424798e+11]]\n",
      "[[4.86229146e+11]]\n",
      "[[2.94147998e+10]]\n",
      "[[8.87261018e+11]]\n",
      "[[9.52818419e+11]]\n",
      "[[5.98706732e+11]]\n",
      "[[9.41760085e+11]]\n",
      "[[4.96257462e+10]]\n",
      "[[4.88477544e+11]]\n",
      "[[4.64162556e+11]]\n",
      "[[2.08806099e+11]]\n",
      "[[1.75808472e+11]]\n",
      "[[6.8738963e+11]]\n",
      "[[1.18324739e+11]]\n",
      "[[6.1630417e+11]]\n",
      "[[9.33292168e+11]]\n",
      "[[7.04118601e+11]]\n",
      "[[5.18278117e+11]]\n",
      "[[2.14274337e+11]]\n",
      "[[5.10364843e+11]]\n",
      "[[1.89121973e+11]]\n",
      "[[9.45977093e+11]]\n",
      "[[4.21203393e+11]]\n",
      "[[4.3326764e+11]]\n",
      "[[4.66362392e+11]]\n",
      "[[8.07974742e+11]]\n",
      "[[2.37763332e+11]]\n",
      "[[5.61024438e+11]]\n",
      "[[9.64436598e+11]]\n",
      "[[6.47477552e+11]]\n",
      "[[8.49505703e+11]]\n",
      "[[4.94611046e+11]]\n",
      "[[2.92084787e+11]]\n",
      "[[7.73310587e+10]]\n",
      "[[9.07996386e+11]]\n",
      "[[6.20370619e+11]]\n",
      "[[2.53589732e+11]]\n",
      "[[2.10381267e+11]]\n",
      "[[6.34296593e+11]]\n",
      "[[9.97830575e+11]]\n",
      "[[8.29608286e+10]]\n",
      "[[8.15815003e+11]]\n",
      "[[7.53658133e+11]]\n",
      "[[2.5860935e+11]]\n",
      "[[5.09470578e+11]]\n",
      "[[5.24284506e+11]]\n",
      "[[2.95334158e+11]]\n",
      "[[8.60103314e+11]]\n",
      "[[9.62852111e+11]]\n",
      "[[2.02612711e+10]]\n",
      "[[3.54984004e+11]]\n",
      "[[9.94825189e+11]]\n",
      "[[4.43085346e+11]]\n",
      "[[9.72990232e+11]]\n",
      "[[4.52363556e+11]]\n"
     ]
    }
   ],
   "source": [
    "#Génération des données\n",
    "\n",
    "# Initialisation des tableaux pour les données\n",
    "all_R_hat_with_phase_complex = np.zeros((num_samples, 2, L, L))\n",
    "all_Z_complex = np.zeros((num_samples, len(G)))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Générer les angles d'arrivée et la matrice X comme décrit précédemment\n",
    "    thetaList_complex = [np.random.uniform(-phi_max, phi_max) for _ in range(nbSources)]  # Exemple de génération d'angles\n",
    "    varList_complex = [np.random.uniform(0, 1000000000000) for _ in range(nbSources)]\n",
    "    X_complex = generate_X_matrix(nbSources, L, T, thetaList_complex, varList_complex, correlation_List, signal_noise_ratio)  # Votre fonction pour générer X, ajoutez les paramètres nécessaires\n",
    "    \n",
    "    # Calculer R_hat étendue avec phase\n",
    "    R_hat_with_phase_complex = generate_R_hat_with_phase_complex(X_complex)\n",
    "    all_R_hat_with_phase_complex[i] = R_hat_with_phase_complex\n",
    "    \n",
    "    # Calculer les labels Z pour les angles générés\n",
    "    all_Z_complex[i] = angles_to_binary_vector(thetaList_complex, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train_complex, X_test_complex, Z_train_complex, Z_test_complex = train_test_split(all_R_hat_with_phase_complex, all_Z_complex, test_size=0.1 , random_state=42)#test_size=0.1 selon papier\n",
    "\n",
    "#Conversion des données en tenseur\n",
    "X_train_complex = torch.from_numpy(X_train_complex).to(dtype=torch.complex64)\n",
    "X_test_complex = torch.from_numpy(X_test_complex).to(dtype=torch.complex64)\n",
    "Z_train_complex = torch.from_numpy(Z_train_complex).to(dtype=torch.complex64)\n",
    "Z_test_complex = torch.from_numpy(Z_test_complex).to(dtype=torch.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du dataset\n",
    "\n",
    "class DOADatasetComplex(Dataset):\n",
    "    def __init__(self, covariance_matrices, doa_labels):\n",
    "        \"\"\"\n",
    "        Initialisation du Dataset DOA.\n",
    "\n",
    "        Args:\n",
    "        - covariance_matrices (np.ndarray): Matrices de covariance étendues avec la phase.\n",
    "          Forme attendue: (nb_samples, nbSensors, nbSensors, 3) pour les parties réelle, imaginaire, et la phase.\n",
    "        - doa_labels (np.ndarray): Labels DOA sous forme de vecteurs binaires.\n",
    "          Forme attendue: (nb_samples, nb_labels), où nb_labels est le nombre de points dans la grille DOA.\n",
    "        \"\"\"\n",
    "        self.covariance_matrices = covariance_matrices\n",
    "        self.doa_labels = doa_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.covariance_matrices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      cov_matrix = self.covariance_matrices[idx]\n",
    "      angle = self.doa_labels[idx]\n",
    "\n",
    "      # Vérification si cov_matrix est déjà un ndarray, sinon le convertir\n",
    "      if not isinstance(cov_matrix, np.ndarray):\n",
    "          cov_matrix = cov_matrix.numpy()  # Convertir en tableau NumPy si ce n'est pas déjà fait\n",
    "    \n",
    "      cov_matrix = torch.from_numpy(cov_matrix).to(dtype=torch.complex64)\n",
    "      angle = torch.tensor(angle).to(dtype=torch.complex64)\n",
    "\n",
    "      return cov_matrix, angle\n",
    "\n",
    "\n",
    "# Exemple d'utilisation (remplacez R_hat_train/test et angles_train/test par vos données réelles)\n",
    "train_dataset_complex = DOADatasetComplex(X_train_complex, Z_train_complex)\n",
    "test_dataset_complex = DOADatasetComplex(X_test_complex, Z_test_complex)\n",
    "\n",
    "# Création des DataLoaders pour l'entraînement et le test, batch_size en fonction de l'article\n",
    "train_dataloader_complex = DataLoader(train_dataset_complex, batch_size=32, shuffle=True)\n",
    "test_dataloader_complex = DataLoader(test_dataset_complex, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOACNNCOMPLEX(\n",
      "  (conv1): Conv2d(2, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv7): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv10): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (relu6): ReLU()\n",
      "  (relu9): ReLU()\n",
      "  (relu12): ReLU()\n",
      "  (relu15): ReLU()\n",
      "  (relu18): ReLU()\n",
      "  (relu21): ReLU()\n",
      "  (dropout16): Dropout(p=0.2, inplace=False)\n",
      "  (dropout19): Dropout(p=0.2, inplace=False)\n",
      "  (dropout22): Dropout(p=0.2, inplace=False)\n",
      "  (flatten13): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc14): Linear(in_features=256, out_features=4096, bias=True)\n",
      "  (fc17): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (fc20): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc23): Linear(in_features=1024, out_features=91, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Définition du modèle\n",
    "\n",
    "class DOACNNCOMPLEX(nn.Module):\n",
    "    def __init__(self, num_channels=2, num_classes=phi_max*2+1):\n",
    "        super(DOACNNCOMPLEX, self).__init__()\n",
    "\n",
    "        # Couches convolutionnelles 2D\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=256, kernel_size=3, stride=2, bias=True)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=2, stride=1, bias=True)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=2, stride=1, bias=True)\n",
    "        self.conv10 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=2, stride=1, bias=True)\n",
    "\n",
    "        # Normalisation de taille 256\n",
    "        self.norm2 = nn.BatchNorm2d(256)\n",
    "        self.norm5 = nn.BatchNorm2d(256)\n",
    "        self.norm8 = nn.BatchNorm2d(256)\n",
    "        self.norm11 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.relu3 =  nn.ReLU()\n",
    "        self.relu6 =  nn.ReLU()\n",
    "        self.relu9 =  nn.ReLU()\n",
    "        self.relu12 =  nn.ReLU()\n",
    "        self.relu15 =  nn.ReLU()\n",
    "        self.relu18 =  nn.ReLU()\n",
    "        self.relu21 =  nn.ReLU()\n",
    "\n",
    "        # Couches Dropout\n",
    "        self.dropout16 = nn.Dropout(0.2)\n",
    "        self.dropout19 = nn.Dropout(0.2)\n",
    "        self.dropout22 = nn.Dropout(0.2)\n",
    "\n",
    "        # Couche d'aplatissement\n",
    "        self.flatten13 = nn.Flatten()\n",
    "\n",
    "        # Couches entièrement connectées (FC)\n",
    "        self.fc14 = nn.Linear(in_features=256 , out_features=4096)\n",
    "        self.fc17 = nn.Linear(in_features=4096, out_features=2048)\n",
    "        self.fc20 = nn.Linear(in_features=2048, out_features=1024)\n",
    "        self.fc23 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "\n",
    "        # Couche de sortie Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm5(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.norm8(x)\n",
    "        x = self.relu9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.norm11(x)\n",
    "        x = self.relu12(x)\n",
    "        x = self.flatten13(x)\n",
    "        x = self.fc14(x)\n",
    "        x = self.relu15(x)\n",
    "        x = self.dropout16(x)\n",
    "        x = self.fc17(x)\n",
    "        x = self.relu18(x)\n",
    "        x = self.dropout19(x)\n",
    "        x = self.fc20(x)\n",
    "        x = self.relu21(x)\n",
    "        x = self.dropout22(x)\n",
    "        x = self.fc23(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Création d'une instance du modèle\n",
    "modelComplex = DOACNNCOMPLEX()\n",
    "print(modelComplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legra\\AppData\\Local\\Temp\\ipykernel_25528\\3472206976.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  angle = torch.tensor(angle).to(dtype=torch.complex64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (struct c10::complex<float>) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelComplex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader_complex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader_complex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntraînement terminé!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     test_loop(test_dataloader, model, loss_fn, device)\n\u001b[0;32m      8\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 45\u001b[0m, in \u001b[0;36mDOACNNCOMPLEX.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(x)\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\legra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (struct c10::complex<float>) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "train(modelComplex, train_dataloader_complex, test_dataloader_complex, loss_fn, optimizer, num_epochs, device)\n",
    "print('Entraînement terminé!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOACNNCOMPLEX(\n",
      "  (conv1): Conv2d(2, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv7): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv10): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (norm11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (relu6): ReLU()\n",
      "  (relu9): ReLU()\n",
      "  (relu12): ReLU()\n",
      "  (relu15): ReLU()\n",
      "  (relu18): ReLU()\n",
      "  (relu21): ReLU()\n",
      "  (dropout16): Dropout(p=0.2, inplace=False)\n",
      "  (dropout19): Dropout(p=0.2, inplace=False)\n",
      "  (dropout22): Dropout(p=0.2, inplace=False)\n",
      "  (flatten13): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc14): Linear(in_features=256, out_features=4096, bias=True)\n",
      "  (fc17): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (fc20): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc23): Linear(in_features=1024, out_features=91, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 4, 4]           4,864\n",
      "       BatchNorm2d-2            [-1, 256, 4, 4]             512\n",
      "              ReLU-3            [-1, 256, 4, 4]               0\n",
      "            Conv2d-4            [-1, 256, 3, 3]         262,400\n",
      "       BatchNorm2d-5            [-1, 256, 3, 3]             512\n",
      "              ReLU-6            [-1, 256, 3, 3]               0\n",
      "            Conv2d-7            [-1, 256, 2, 2]         262,400\n",
      "       BatchNorm2d-8            [-1, 256, 2, 2]             512\n",
      "              ReLU-9            [-1, 256, 2, 2]               0\n",
      "           Conv2d-10            [-1, 256, 1, 1]         262,400\n",
      "      BatchNorm2d-11            [-1, 256, 1, 1]             512\n",
      "             ReLU-12            [-1, 256, 1, 1]               0\n",
      "          Flatten-13                  [-1, 256]               0\n",
      "           Linear-14                 [-1, 4096]       1,052,672\n",
      "             ReLU-15                 [-1, 4096]               0\n",
      "          Dropout-16                 [-1, 4096]               0\n",
      "           Linear-17                 [-1, 2048]       8,390,656\n",
      "             ReLU-18                 [-1, 2048]               0\n",
      "          Dropout-19                 [-1, 2048]               0\n",
      "           Linear-20                 [-1, 1024]       2,098,176\n",
      "             ReLU-21                 [-1, 1024]               0\n",
      "          Dropout-22                 [-1, 1024]               0\n",
      "           Linear-23                   [-1, 91]          93,275\n",
      "          Sigmoid-24                   [-1, 91]               0\n",
      "================================================================\n",
      "Total params: 12,428,891\n",
      "Trainable params: 12,428,891\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.34\n",
      "Params size (MB): 47.41\n",
      "Estimated Total Size (MB): 47.76\n",
      "----------------------------------------------------------------\n",
      "Layer: conv1.weight | Size: torch.Size([256, 2, 3, 3])\n",
      "Layer: conv1.bias | Size: torch.Size([256])\n",
      "Layer: conv4.weight | Size: torch.Size([256, 256, 2, 2])\n",
      "Layer: conv4.bias | Size: torch.Size([256])\n",
      "Layer: conv7.weight | Size: torch.Size([256, 256, 2, 2])\n",
      "Layer: conv7.bias | Size: torch.Size([256])\n",
      "Layer: conv10.weight | Size: torch.Size([256, 256, 2, 2])\n",
      "Layer: conv10.bias | Size: torch.Size([256])\n",
      "Layer: norm2.weight | Size: torch.Size([256])\n",
      "Layer: norm2.bias | Size: torch.Size([256])\n",
      "Layer: norm5.weight | Size: torch.Size([256])\n",
      "Layer: norm5.bias | Size: torch.Size([256])\n",
      "Layer: norm8.weight | Size: torch.Size([256])\n",
      "Layer: norm8.bias | Size: torch.Size([256])\n",
      "Layer: norm11.weight | Size: torch.Size([256])\n",
      "Layer: norm11.bias | Size: torch.Size([256])\n",
      "Layer: fc14.weight | Size: torch.Size([4096, 256])\n",
      "Layer: fc14.bias | Size: torch.Size([4096])\n",
      "Layer: fc17.weight | Size: torch.Size([2048, 4096])\n",
      "Layer: fc17.bias | Size: torch.Size([2048])\n",
      "Layer: fc20.weight | Size: torch.Size([1024, 2048])\n",
      "Layer: fc20.bias | Size: torch.Size([1024])\n",
      "Layer: fc23.weight | Size: torch.Size([91, 1024])\n",
      "Layer: fc23.bias | Size: torch.Size([91])\n"
     ]
    }
   ],
   "source": [
    "#Quelques informations sur notre modèle : \n",
    "\n",
    "print(modelComplex)\n",
    "\n",
    "summary(modelComplex, input_size=(2, nbSensors, nbSensors))\n",
    "\n",
    "#Poids des paramètres :\n",
    "\n",
    "\n",
    "# Initialiser le compteur de paramètres\n",
    "total_params_complex = 0\n",
    "\n",
    "# Boucle à travers les paramètres de votre modèle\n",
    "for param in modelComplex.parameters():\n",
    "    # Vérifier si le paramètre nécessite un gradient (c'est-à-dire s'il est entraînable)\n",
    "    if param.requires_grad:\n",
    "        # Compter le nombre de valeurs dans le paramètre\n",
    "        total_params_complex += param.numel()\n",
    "\n",
    "# Boucle à travers les paramètres de votre modèle\n",
    "for name, param in modelComplex.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
