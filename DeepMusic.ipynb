{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Signal_generator.generate_signal import generate_X_matrix\n",
    "from Algorithmes.beamforming import beamforming_method\n",
    "from Signal_generator.generate_signal import generate_A_matrix\n",
    "from Signal_generator.generate_signal import generate_S_matrix\n",
    "from Signal_generator.generate_signal import generate_noise\n",
    "from Signal_generator.generate_signal import generate_R_hat\n",
    "from Signal_generator.generate_signal import generate_R_hat_with_phase\n",
    "from Algorithmes.music import music_method\n",
    "from Algorithmes.music import estimate_angles\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torchsummary import summary\n",
    "from numpy.linalg import eigh\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from Algorithmes.music import generate_steering_vector\n",
    "from Plots.draw_plot import plot_single_music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_music_spectrum(R_hat, nbSensors, nbSources, angles_range):\n",
    "    # Calculer les vecteurs propres et les valeurs propres\n",
    "    _, eigenvectors = np.linalg.eigh(R_hat)\n",
    "    # Sélectionner les k plus grandes valeurs propres\n",
    "    noise_subspace = eigenvectors[:, :-nbSources]\n",
    "    music_spectrum = np.zeros_like(angles_range, dtype=float)\n",
    "    for idx, theta in enumerate(angles_range):\n",
    "        steering_vector = generate_steering_vector(nbSensors, theta)\n",
    "        music_spectrum[idx] = 1 / np.linalg.norm(noise_subspace.conj().T @ steering_vector)\n",
    "\n",
    "    return music_spectrum\n",
    "\n",
    "perturbation_parameter_sd = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_combinations(phi_max, rho, nb_sources):\n",
    "    \"\"\"Cette fonction sert à générer toutes les combinaisons d'angles possibles dans la plage de résolution\"\"\"\n",
    "    # Générer une plage de valeurs possibles pour les signaux\n",
    "    values = list(range(-int(phi_max / rho), int(phi_max / rho) + 1))\n",
    "    \n",
    "    # Générer toutes les combinaisons possibles de signaux\n",
    "    all_combinations = list(itertools.product(values, repeat=nb_sources))\n",
    "    \n",
    "    # Supprimer les combinaisons où l'ordre ne compte pas et où deux signaux ont la même valeur\n",
    "    unique_combinations = {tuple(sorted(combination)) for combination in all_combinations if len(set(combination)) == nb_sources}\n",
    "    \n",
    "    return list(unique_combinations)\n",
    "\n",
    "\n",
    "Q = 2\n",
    "N = 121\n",
    "phi_max = 60\n",
    "rho = 1\n",
    "nbSources = 2 # Nombre de sources\n",
    "nbSensors = 9 # Nombre de capteurs\n",
    "nbTimePoints = 100 # Nombre de points temporels\n",
    "signal_noise_ratio = 10 # Rapport signal sur bruit en décibels. Si 'False', cela revient à une absence totale de bruit.\n",
    "theta1 = -20 # Angle entre la perpendiculaire à la ligne de capteurs, et la source 1\n",
    "theta2 = 20 # Angle entre la perpendiculaire à la ligne de capteurs, et la source 2\n",
    "var_ratio = [1] # Liste qui donne le rapport entre la variance du signal 1 et celui des autres sources (ex: [2, 3] signifie que la source 2 a une variance 2 fois plus grande que la source 1, et la source 3 a une variance 3 fois plus grande que la source 1)\n",
    "correlation_List = [0] # Liste des corrélations. Il y a une corrélation nécéssaire pour chaque paire distincte de sources différentes: 0 pour 1 source, 1 pour 2 sources, 3 pour 3 sources, 6 pour 4 sources etc...\n",
    "# Ordre de remplisage de la correlation_List: de gauche à droite et ligne par ligne, moitié haut-droite de la matrice uniquement, puis symétrie de ces valeurs pour la moitié bas-gauche\n",
    "perturbation_parameter_sd = 0 # Écart-type de la distribution normale qui génère les erreurs de calibration des capteurs\n",
    "\n",
    "\n",
    "def generate_deepmusic_partitioned_data(nbSensors, nbSources, T, SNR_TRAIN, Q, N, phi_max, rho, correlation_List, var_ratio):\n",
    "    angles_combination = generate_combinations(phi_max, rho, nbSources)\n",
    "    training_data_sets = [[] for _ in range(Q)]\n",
    "    L = N // Q  # Assuming N is divisible by Q\n",
    "\n",
    "    i = 0\n",
    "    for signal_noise_ratio in SNR_TRAIN:\n",
    "        for angles in angles_combination:\n",
    "            thetaList = list(angles)\n",
    "\n",
    "            X = generate_X_matrix(nbSources=nbSources, nbSensors=nbSensors, nbTimePoints=nbTimePoints, thetaList=thetaList, var_ratio=var_ratio, correlation_List=correlation_List, signal_noise_ratio=signal_noise_ratio, perturbation_parameter_sd=perturbation_parameter_sd)\n",
    "            R_hat_with_phase = generate_R_hat_with_phase(X)\n",
    "            R_hat = generate_R_hat(X)\n",
    "\n",
    "            full_theta = np.linspace(- phi_max , phi_max, N)\n",
    "            full_music_spectrum = calculate_music_spectrum(R_hat=R_hat,nbSensors=nbSensors, nbSources=nbSources, angles_range=full_theta)\n",
    "\n",
    "            for q in range(Q):\n",
    "                # Sampling MUSIC spectrum for the q-th subregion\n",
    "                start_index = q * L\n",
    "                end_index = (q + 1)*L if q != Q - 1 else N  # Adjusted this line\n",
    "                pq = full_music_spectrum[start_index:end_index]\n",
    "                \n",
    "                input_data = R_hat_with_phase\n",
    "                output_data = pq\n",
    "                training_data_sets[q].append((input_data, output_data))\n",
    "            i += 1\n",
    "\n",
    "    return training_data_sets\n",
    "\n",
    "full_theta = np.linspace(- phi_max , phi_max, N)\n",
    "# Example usage\n",
    "SNR_TRAIN = [-20,-15,-10, -5, 0] # Different SNR levels for training\n",
    "training_datasets = generate_deepmusic_partitioned_data(nbSensors=nbSensors, nbSources=nbSources, T=nbTimePoints, SNR_TRAIN=SNR_TRAIN, Q=Q, N=N, phi_max=phi_max, rho=rho, correlation_List=correlation_List, var_ratio=var_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352.0\n",
      "23232\n",
      "88.0\n",
      "5808\n",
      "110.0\n",
      "7260\n",
      "352.0\n",
      "23232\n",
      "88.0\n",
      "5808\n",
      "110.0\n",
      "7260\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DeepMusicDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data, output_data = self.data[idx]\n",
    "        return torch.tensor(input_data, dtype=torch.float32), torch.tensor(output_data, dtype=torch.float32)\n",
    "\n",
    "def create_loaders_for_subregion(data, batch_size=66, validation_split=0.2, test_split=0.2):\n",
    "    # Séparation des données en ensembles d'entraînement, de validation et de test\n",
    "    train_data, test_data = train_test_split(data, test_size=test_split, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=validation_split, random_state=42)\n",
    "\n",
    "    # Création des instances de DeepMusicDataset\n",
    "    train_dataset = DeepMusicDataset(train_data)\n",
    "    print(len(train_dataset)/batch_size)\n",
    "    print(len(train_dataset))\n",
    "    val_dataset = DeepMusicDataset(val_data)\n",
    "    print(len(val_dataset)/batch_size)\n",
    "    print(len(val_dataset))\n",
    "    test_dataset = DeepMusicDataset(test_data)\n",
    "    print(len(test_dataset)/batch_size)\n",
    "    print(len(test_dataset))\n",
    "\n",
    "    # Création des DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Création des DataLoader pour chaque sous-région\n",
    "loaders_per_subregion = [create_loaders_for_subregion(training_datasets[q]) for q in range(Q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMusicModel(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(DeepMusicModel, self).__init__()\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Define the fourth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        # Assuming the spatial dimensions (height and width) are reduced to 1x1 after the convolutions\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=20736, out_features=output_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Softmax layer \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Apply the second convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Apply the third convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Apply the fourth convolutional layer and normalization, followed by ReLU\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Apply the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Apply the dropout layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #Apply the softmax layer\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#Définition du device\n",
    "\n",
    "device = (\n",
    "    \"gpu\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 4\n",
      "Early stopping at epoch 7\n",
      "Training time: 331.74193024635315 seconds\n",
      "Average MSE per epoch on validation set: [0.3199742312373763, 0.3199869720688598, 0.3199889477850361, 0.3199892654506998, 0.3194872811436653, 0.31946825655177236, 0.31947814182124357, 0.3194857950068333, 0.31947055780752137, 0.3194822702895511, 0.31947106893428345, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Average MSE per epoch on training set: [0.32533201710744336, 0.32533652166073973, 0.3253329465673728, 0.3253348335962404, 0.3163617224517194, 0.3163614259524779, 0.316362999887629, 0.3163610024547035, 0.31636252101849427, 0.3163640516048128, 0.3163618558848446, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Average Training Loss: 0.3197\n",
      "Average Validation Loss: 0.3196\n",
      "MSE on Test Set: 0.3168\n",
      "Training time: 331.74193024635315\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_accuracy(predicted_spectrum, target_spectrum, angles_range, nbSources, peak_tolerance):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for a single predicted and target spectrum.\n",
    "    \n",
    "    :param predicted_spectrum: Predicted MUSIC spectrum (1D NumPy array).\n",
    "    :param target_spectrum: True MUSIC spectrum (1D NumPy array).\n",
    "    :param angles_range: The range of angles over which the MUSIC spectrum is calculated.\n",
    "    :param nbSources: The number of sources (peaks) to consider.\n",
    "    :param peak_tolerance: The tolerance for considering a predicted peak to match a target peak.\n",
    "    :return: Accuracy as a float.\n",
    "    \"\"\"\n",
    "    # Estimate angles from the predicted and target spectra\n",
    "    estimated_angles = estimate_angles(nbSources, predicted_spectrum, angles_range)\n",
    "    true_angles = estimate_angles(nbSources, target_spectrum, angles_range)\n",
    "\n",
    "    # Compare the estimated angles with the true angles\n",
    "    correct_predictions = sum(1 for est_angle in estimated_angles if np.any(np.abs(true_angles - est_angle) <= peak_tolerance))\n",
    "    accuracy = correct_predictions / nbSources if nbSources > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_spectrum(loaders_per_subregion, epochs, nbSources, angles_range, peak_tolerance=5):\n",
    "    models = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_mse_list = []\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_mse': [],\n",
    "        'avg_mse_per_epoch_val': [],\n",
    "        'avg_mse_per_epoch_train': [],\n",
    "        'total_mse': None,\n",
    "        'training_time': None\n",
    "    }\n",
    "\n",
    "    # Training configuration\n",
    "    initial_lr = 0.01\n",
    "    lr_decay_factor = 0.5\n",
    "    lr_decay_epoch = 10\n",
    "    early_stopping_patience = 3\n",
    "\n",
    "    start_time = time.time()  # Start time for training\n",
    "    i = 0\n",
    "    for train_loader, val_loader, test_loader in loaders_per_subregion:\n",
    "        if i == 0 :\n",
    "            model = DeepMusicModel(N//Q)\n",
    "        else: \n",
    "            model = DeepMusicModel(N//Q + 1)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        best_val_mse = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Adjust learning rate\n",
    "            if epoch % lr_decay_epoch == 0 and epoch > 0:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= lr_decay_factor\n",
    "\n",
    "            # Training loop\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets).item()\n",
    "                optimizer.step()\n",
    "                train_loss += loss\n",
    "                \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_loss_list.append(avg_train_loss)\n",
    "            metrics[\"train_loss\"].append(avg_train_loss)\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss, val_mse = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += criterion(outputs, targets).item()\n",
    "                    val_mse += F.mse_loss(outputs, targets).item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_val_mse = val_mse / len(val_loader)\n",
    "            val_loss_list.append(avg_val_loss)\n",
    "            metrics[\"val_loss\"].append(avg_val_loss)\n",
    "            val_mse_list.append(avg_val_mse)\n",
    "            metrics[\"val_mse\"].append(avg_val_mse)\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_mse < best_val_mse:\n",
    "                best_val_mse = avg_val_mse\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve == early_stopping_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        models.append(model)\n",
    "        i += 1\n",
    "\n",
    "    # Calculate training time\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training time: {training_time} seconds\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    concatenated_outputs = []\n",
    "    concatenated_targets = []\n",
    "\n",
    "    for model, test_loader in zip(models, [x[2] for x in loaders_per_subregion]):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Concatenate outputs and targets\n",
    "                concatenated_outputs.append(outputs)\n",
    "                concatenated_targets.append(targets)\n",
    "\n",
    "    # Calculate the average MSE per epoch on the validation set\n",
    "    avg_mse_per_epoch_val = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_mse = []\n",
    "        for i, model in enumerate(models):\n",
    "            # Determine the actual number of epochs the model was trained for\n",
    "            actual_epochs = len(train_loss_list[i * epochs:(i + 1) * epochs])\n",
    "            if epoch < actual_epochs:\n",
    "                epoch_mse.append(train_loss_list[i * epochs + epoch])\n",
    "        avg_mse_per_epoch_val.append(np.mean(epoch_mse))\n",
    "        metrics[\"avg_mse_per_epoch_train\"].append(np.mean(epoch_mse))\n",
    "        \n",
    "\n",
    "    print(\"Average MSE per epoch on validation set:\", avg_mse_per_epoch_val)\n",
    "\n",
    "    # Calculate the average MSE per epoch on the training set\n",
    "    avg_mse_per_epoch_train = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_mse = []\n",
    "        for i, model in enumerate(models):\n",
    "            # Determine the actual number of epochs the model was trained for\n",
    "            actual_epochs = len(val_mse_list[i * epochs:(i + 1) * epochs])\n",
    "            if epoch < actual_epochs:\n",
    "                epoch_mse.append(val_mse_list[i * epochs + epoch])\n",
    "        avg_mse_per_epoch_train.append(np.mean(epoch_mse))\n",
    "        metrics[\"avg_mse_per_epoch_train\"].append(np.mean(epoch_mse))\n",
    "\n",
    "    print(\"Average MSE per epoch on training set:\", avg_mse_per_epoch_train)\n",
    "\n",
    "    # Determine the maximum size of all targets\n",
    "    max_size = max(target.size(1) for target in concatenated_targets)\n",
    "    # Pad each target tensor to the maximum size\n",
    "    padded_targets = [F.pad(target, (0, max_size - target.size(1))) for target in concatenated_targets]\n",
    "\n",
    "    padded_outputs = [F.pad(output, (0, max_size - output.size(1))) for output in concatenated_outputs]\n",
    "\n",
    "    # Concatenate the padded targets into a single tensor\n",
    "    concatenated_targets = torch.cat(padded_targets, dim=0)\n",
    "    concatenated_outputs = torch.cat(padded_outputs, dim=0)\n",
    "\n",
    "    # Compute MSE for concatenated outputs and targets\n",
    "    total_mse = F.mse_loss(concatenated_outputs, concatenated_targets).item()\n",
    "    metrics['total_mse'] = total_mse\n",
    "    metrics['training_time'] = training_time\n",
    "    print(f'Average Training Loss: {np.mean(train_loss_list):.4f}')\n",
    "    print(f'Average Validation Loss: {np.mean(val_loss_list):.4f}')\n",
    "    print(f'MSE on Test Set: {total_mse:.4f}')\n",
    "    print(f'Training time: {training_time}')\n",
    "    return models, metrics\n",
    "# Example usage\n",
    "model1_results = train_and_evaluate_spectrum(loaders_per_subregion=loaders_per_subregion, epochs=1000, nbSources=nbSources, angles_range=full_theta, peak_tolerance=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.compare import *\n",
    "models, _ = model1_results\n",
    "save_models_hyperparams_and_metadata(models, hyperparameters_list=[{\"A\" : \"a\"} for _ in range(Q)], metadata_list=[{\"A\" : \"a\"} for _ in range(Q)], directory_name=\"DeepMusic_subregion_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeepMusicModel(\n",
       "   (conv1): Conv2d(3, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "   (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "   (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (fc1): Linear(in_features=20736, out_features=60, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (softmax): Softmax(dim=1)\n",
       " ),\n",
       " DeepMusicModel(\n",
       "   (conv1): Conv2d(3, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "   (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "   (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (fc1): Linear(in_features=20736, out_features=61, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       "   (softmax): Softmax(dim=1)\n",
       " )]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
